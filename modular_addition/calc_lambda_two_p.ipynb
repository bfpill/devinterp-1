{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "2 3\n",
      "2 5\n",
      "2 7\n",
      "2 11\n",
      "2 13\n",
      "2 17\n",
      "2 19\n",
      "2 23\n",
      "2 29\n",
      "2 31\n",
      "2 37\n",
      "2 41\n",
      "2 43\n",
      "2 47\n",
      "2 53\n",
      "2 59\n",
      "2 61\n",
      "2 67\n",
      "2 71\n",
      "2 73\n",
      "2 79\n",
      "2 83\n",
      "2 89\n",
      "2 97\n",
      "3 2\n",
      "3 3\n",
      "3 5\n",
      "3 7\n",
      "3 11\n",
      "3 13\n",
      "3 17\n",
      "3 19\n",
      "3 23\n",
      "3 29\n",
      "3 31\n",
      "3 37\n",
      "3 41\n",
      "3 43\n",
      "3 47\n",
      "3 53\n",
      "3 59\n",
      "3 61\n",
      "3 67\n",
      "3 71\n",
      "3 73\n",
      "3 79\n",
      "3 83\n",
      "3 89\n",
      "3 97\n",
      "5 2\n",
      "5 3\n",
      "5 5\n",
      "5 7\n",
      "5 11\n",
      "5 13\n",
      "5 17\n",
      "5 19\n",
      "5 23\n",
      "5 29\n",
      "5 31\n",
      "5 37\n",
      "5 41\n",
      "5 43\n",
      "5 47\n",
      "5 53\n",
      "5 59\n",
      "5 61\n",
      "5 67\n",
      "5 71\n",
      "5 73\n",
      "5 79\n",
      "5 83\n",
      "5 89\n",
      "5 97\n",
      "7 2\n",
      "7 3\n",
      "7 5\n",
      "7 7\n",
      "7 11\n",
      "7 13\n",
      "7 17\n",
      "7 19\n",
      "7 23\n",
      "7 29\n",
      "7 31\n",
      "7 37\n",
      "7 41\n",
      "7 43\n",
      "7 47\n",
      "7 53\n",
      "7 59\n",
      "7 61\n",
      "7 67\n",
      "7 71\n",
      "7 73\n",
      "7 79\n",
      "7 83\n",
      "7 89\n",
      "7 97\n",
      "11 2\n",
      "11 3\n",
      "11 5\n",
      "11 7\n",
      "11 11\n",
      "11 13\n",
      "11 17\n",
      "11 19\n",
      "11 23\n",
      "11 29\n",
      "11 31\n",
      "11 37\n",
      "11 41\n",
      "11 43\n",
      "11 47\n",
      "11 53\n",
      "11 59\n",
      "11 61\n",
      "11 67\n",
      "11 71\n",
      "11 73\n",
      "11 79\n",
      "11 83\n",
      "11 89\n",
      "11 97\n",
      "13 2\n",
      "13 3\n",
      "13 5\n",
      "13 7\n",
      "13 11\n",
      "13 13\n",
      "13 17\n",
      "13 19\n",
      "13 23\n",
      "13 29\n",
      "13 31\n",
      "13 37\n",
      "13 41\n",
      "13 43\n",
      "13 47\n",
      "13 53\n",
      "13 59\n",
      "13 61\n",
      "13 67\n",
      "13 71\n",
      "13 73\n",
      "13 79\n",
      "13 83\n",
      "13 89\n",
      "13 97\n",
      "17 2\n",
      "17 3\n",
      "17 5\n",
      "17 7\n",
      "17 11\n",
      "17 13\n",
      "17 17\n",
      "17 19\n",
      "17 23\n",
      "17 29\n",
      "17 31\n",
      "17 37\n",
      "17 41\n",
      "17 43\n",
      "17 47\n",
      "17 53\n",
      "17 59\n",
      "17 61\n",
      "17 67\n",
      "17 71\n",
      "17 73\n",
      "17 79\n",
      "17 83\n",
      "17 89\n",
      "17 97\n",
      "19 2\n",
      "19 3\n",
      "19 5\n",
      "19 7\n",
      "19 11\n",
      "19 13\n",
      "19 17\n",
      "19 19\n",
      "19 23\n",
      "19 29\n",
      "19 31\n",
      "19 37\n",
      "19 41\n",
      "19 43\n",
      "19 47\n",
      "19 53\n",
      "19 59\n",
      "19 61\n",
      "19 67\n",
      "19 71\n",
      "19 73\n",
      "19 79\n",
      "19 83\n",
      "19 89\n",
      "19 97\n",
      "23 2\n",
      "23 3\n",
      "23 5\n",
      "23 7\n",
      "23 11\n",
      "23 13\n",
      "23 17\n",
      "23 19\n",
      "23 23\n",
      "23 29\n",
      "23 31\n",
      "23 37\n",
      "23 41\n",
      "23 43\n",
      "23 47\n",
      "23 53\n",
      "23 59\n",
      "23 61\n",
      "23 67\n",
      "23 71\n",
      "23 73\n",
      "23 79\n",
      "23 83\n",
      "23 89\n",
      "23 97\n",
      "29 2\n",
      "29 3\n",
      "29 5\n",
      "29 7\n",
      "29 11\n",
      "29 13\n",
      "29 17\n",
      "29 19\n",
      "29 23\n",
      "29 29\n",
      "29 31\n",
      "29 37\n",
      "29 41\n",
      "29 43\n",
      "29 47\n",
      "29 53\n",
      "29 59\n",
      "29 61\n",
      "29 67\n",
      "29 71\n",
      "29 73\n",
      "29 79\n",
      "29 83\n",
      "29 89\n",
      "29 97\n",
      "31 2\n",
      "31 3\n",
      "31 5\n",
      "31 7\n",
      "31 11\n",
      "31 13\n",
      "31 17\n",
      "31 19\n",
      "31 23\n",
      "31 29\n",
      "31 31\n",
      "31 37\n",
      "31 41\n",
      "31 43\n",
      "31 47\n",
      "31 53\n",
      "31 59\n",
      "31 61\n",
      "31 67\n",
      "31 71\n",
      "31 73\n",
      "31 79\n",
      "31 83\n",
      "31 89\n",
      "31 97\n",
      "37 2\n",
      "37 3\n",
      "37 5\n",
      "37 7\n",
      "37 11\n",
      "37 13\n",
      "37 17\n",
      "37 19\n",
      "37 23\n",
      "37 29\n",
      "37 31\n",
      "37 37\n",
      "37 41\n",
      "37 43\n",
      "37 47\n",
      "37 53\n",
      "37 59\n",
      "37 61\n",
      "37 67\n",
      "37 71\n",
      "37 73\n",
      "37 79\n",
      "37 83\n",
      "37 89\n",
      "37 97\n",
      "41 2\n",
      "41 3\n",
      "41 5\n",
      "41 7\n",
      "41 11\n",
      "41 13\n",
      "41 17\n",
      "41 19\n",
      "41 23\n",
      "41 29\n",
      "41 31\n",
      "41 37\n",
      "41 41\n",
      "41 43\n",
      "41 47\n",
      "41 53\n",
      "41 59\n",
      "41 61\n",
      "41 67\n",
      "41 71\n",
      "41 73\n",
      "41 79\n",
      "41 83\n",
      "41 89\n",
      "41 97\n",
      "43 2\n",
      "43 3\n",
      "43 5\n",
      "43 7\n",
      "43 11\n",
      "43 13\n",
      "43 17\n",
      "43 19\n",
      "43 23\n",
      "43 29\n",
      "43 31\n",
      "43 37\n",
      "43 41\n",
      "43 43\n",
      "43 47\n",
      "43 53\n",
      "43 59\n",
      "43 61\n",
      "43 67\n",
      "43 71\n",
      "43 73\n",
      "43 79\n",
      "43 83\n",
      "43 89\n",
      "43 97\n",
      "47 2\n",
      "47 3\n",
      "47 5\n",
      "47 7\n",
      "47 11\n",
      "47 13\n",
      "47 17\n",
      "47 19\n",
      "47 23\n",
      "47 29\n",
      "47 31\n",
      "47 37\n",
      "47 41\n",
      "47 43\n",
      "47 47\n",
      "47 53\n",
      "47 59\n",
      "47 61\n",
      "47 67\n",
      "47 71\n",
      "47 73\n",
      "47 79\n",
      "47 83\n",
      "47 89\n",
      "47 97\n",
      "53 2\n",
      "53 3\n",
      "53 5\n",
      "53 7\n",
      "53 11\n",
      "53 13\n",
      "53 17\n",
      "53 19\n",
      "53 23\n",
      "53 29\n",
      "53 31\n",
      "53 37\n",
      "53 41\n",
      "53 43\n",
      "53 47\n",
      "53 53\n",
      "53 59\n",
      "53 61\n",
      "53 67\n",
      "53 71\n",
      "53 73\n",
      "53 79\n",
      "53 83\n",
      "53 89\n",
      "53 97\n",
      "59 2\n",
      "59 3\n",
      "59 5\n",
      "59 7\n",
      "59 11\n",
      "59 13\n",
      "59 17\n",
      "59 19\n",
      "59 23\n",
      "59 29\n",
      "59 31\n",
      "59 37\n",
      "59 41\n",
      "59 43\n",
      "59 47\n",
      "59 53\n",
      "59 59\n",
      "59 61\n",
      "59 67\n",
      "59 71\n",
      "59 73\n",
      "59 79\n",
      "59 83\n",
      "59 89\n",
      "59 97\n",
      "61 2\n",
      "61 3\n",
      "61 5\n",
      "61 7\n",
      "61 11\n",
      "61 13\n",
      "61 17\n",
      "61 19\n",
      "61 23\n",
      "61 29\n",
      "61 31\n",
      "61 37\n",
      "61 41\n",
      "61 43\n",
      "61 47\n",
      "61 53\n",
      "61 59\n",
      "61 61\n",
      "61 67\n",
      "61 71\n",
      "61 73\n",
      "61 79\n",
      "61 83\n",
      "61 89\n",
      "61 97\n",
      "67 2\n",
      "67 3\n",
      "67 5\n",
      "67 7\n",
      "67 11\n",
      "67 13\n",
      "67 17\n",
      "67 19\n",
      "67 23\n",
      "67 29\n",
      "67 31\n",
      "67 37\n",
      "67 41\n",
      "67 43\n",
      "67 47\n",
      "67 53\n",
      "67 59\n",
      "67 61\n",
      "67 67\n",
      "67 71\n",
      "67 73\n",
      "67 79\n",
      "67 83\n",
      "67 89\n",
      "67 97\n",
      "71 2\n",
      "71 3\n",
      "71 5\n",
      "71 7\n",
      "71 11\n",
      "71 13\n",
      "71 17\n",
      "71 19\n",
      "71 23\n",
      "71 29\n",
      "71 31\n",
      "71 37\n",
      "71 41\n",
      "71 43\n",
      "71 47\n",
      "71 53\n",
      "71 59\n",
      "71 61\n",
      "71 67\n",
      "71 71\n",
      "71 73\n",
      "71 79\n",
      "71 83\n",
      "71 89\n",
      "71 97\n",
      "73 2\n",
      "73 3\n",
      "73 5\n",
      "73 7\n",
      "73 11\n",
      "73 13\n",
      "73 17\n",
      "73 19\n",
      "73 23\n",
      "73 29\n",
      "73 31\n",
      "73 37\n",
      "73 41\n",
      "73 43\n",
      "73 47\n",
      "73 53\n",
      "73 59\n",
      "73 61\n",
      "73 67\n",
      "73 71\n",
      "73 73\n",
      "73 79\n",
      "73 83\n",
      "73 89\n",
      "73 97\n",
      "79 2\n",
      "79 3\n",
      "79 5\n",
      "79 7\n",
      "79 11\n",
      "79 13\n",
      "79 17\n",
      "79 19\n",
      "79 23\n",
      "79 29\n",
      "79 31\n",
      "79 37\n",
      "79 41\n",
      "79 43\n",
      "79 47\n",
      "79 53\n",
      "79 59\n",
      "79 61\n",
      "79 67\n",
      "79 71\n",
      "79 73\n",
      "79 79\n",
      "79 83\n",
      "79 89\n",
      "79 97\n",
      "83 2\n",
      "83 3\n",
      "83 5\n",
      "83 7\n",
      "83 11\n",
      "83 13\n",
      "83 17\n",
      "83 19\n",
      "83 23\n",
      "83 29\n",
      "83 31\n",
      "83 37\n",
      "83 41\n",
      "83 43\n",
      "83 47\n",
      "83 53\n",
      "83 59\n",
      "83 61\n",
      "83 67\n",
      "83 71\n",
      "83 73\n",
      "83 79\n",
      "83 83\n",
      "83 89\n",
      "83 97\n",
      "89 2\n",
      "89 3\n",
      "89 5\n",
      "89 7\n",
      "89 11\n",
      "89 13\n",
      "89 17\n",
      "89 19\n",
      "89 23\n",
      "89 29\n",
      "89 31\n",
      "89 37\n",
      "89 41\n",
      "89 43\n",
      "89 47\n",
      "89 53\n",
      "89 59\n",
      "89 61\n",
      "89 67\n",
      "89 71\n",
      "89 73\n",
      "89 79\n",
      "89 83\n",
      "89 89\n",
      "89 97\n",
      "97 2\n",
      "97 3\n",
      "97 5\n",
      "97 7\n",
      "97 11\n",
      "97 13\n",
      "97 17\n",
      "97 19\n",
      "97 23\n",
      "97 29\n",
      "97 31\n",
      "97 37\n",
      "97 41\n",
      "97 43\n",
      "97 47\n",
      "97 53\n",
      "97 59\n",
      "97 61\n",
      "97 67\n",
      "97 71\n",
      "97 73\n",
      "97 79\n",
      "97 83\n",
      "97 89\n",
      "97 97\n"
     ]
    }
   ],
   "source": [
    "from train_two_p import ExperimentParamsTwoP\n",
    "import torch as t\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from helpers import get_submodule_param_mask\n",
    "from math import log, sqrt\n",
    "from two_p_model import TwoPMLP\n",
    "from two_p_dataset import make_two_p_dataset, train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Optional\n",
    "import json\n",
    "from glob import glob\n",
    "from model_viz import viz_weights_modes\n",
    "from movie import run_movie_cmd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from dynamics import get_magnitude_modes\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SGLDParams:\n",
    "    gamma: float = 1\n",
    "    epsilon: float = 0.001\n",
    "    n_steps: int = 10000\n",
    "    m: int = 512  # SGLD batch size\n",
    "    restrict_to_orth_grad: bool = False\n",
    "    get_updated_model_parameters: Callable = lambda model: model.parameters()  # Override to only search parameter subspace\n",
    "    n_multiplier: float = 1\n",
    "    movie: bool = False\n",
    "    num_point_samples: Optional[int] = None\n",
    "    n_magnitude_samples: Optional[int] = None\n",
    "    weight_decay: float = 0\n",
    "    logit_scaling: float = 1.0\n",
    "    temp_multiplier: float = 1.0\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, y_s, logit_scaling=1):\n",
    "    \"\"\"\n",
    "    logits: outputs of model\n",
    "    y: target labels\n",
    "\n",
    "    returns: mean cross entropy loss\n",
    "    \"\"\"\n",
    "    preds = t.nn.functional.softmax(logits * logit_scaling, dim=1)\n",
    "    return -1 * t.mean(t.log(preds[t.arange(len(preds)), y_s] + 1e-7))\n",
    "\n",
    "\n",
    "def get_full_train_loss(model, dataset, device, logit_scaling=1):\n",
    "    model = model.to(device)\n",
    "    X1 = t.stack([sample[0] for sample in dataset]).to(device)\n",
    "    X2 = t.stack([sample[1] for sample in dataset]).to(device)\n",
    "    X3 = t.stack([sample[2] for sample in dataset]).to(device)\n",
    "    X4 = t.stack([sample[3] for sample in dataset]).to(device)\n",
    "    Y1 = t.stack([sample[4] for sample in dataset]).to(device)\n",
    "    Y2 = t.stack([sample[5] for sample in dataset]).to(device)\n",
    "\n",
    "    \n",
    "    seeds = t.rand(Y1.size(0), device=device)\n",
    "    mask = seeds < 0.5\n",
    "    labels = Y1.clone()\n",
    "    labels[~mask] = Y2[~mask]\n",
    "    \n",
    "    out = model(X1, X2, X3, X4)\n",
    "    loss = cross_entropy_loss(out, labels, logit_scaling=logit_scaling)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def mean(arr):\n",
    "    return sum(arr) / len(arr)\n",
    "\n",
    "\n",
    "def sgld(model, sgld_params, dataset, device):\n",
    "    \"\"\"\n",
    "    model: MLP model\n",
    "    sgld_params: SGLDParams object\n",
    "    dataset: dataset to train on\n",
    "    device: device to run on\n",
    "\n",
    "    returns: updated model, lambda_hat\n",
    "    \"\"\"\n",
    "    n = len(dataset)\n",
    "    model = model.to(device)\n",
    "    effective_n = n * sgld_params.n_multiplier\n",
    "\n",
    "    inverse_temp = effective_n / log(effective_n)\n",
    "    inverse_temp /= sgld_params.temp_multiplier\n",
    "\n",
    "    init_loss = get_full_train_loss(\n",
    "        model, dataset, device, logit_scaling=sgld_params.logit_scaling\n",
    "    )\n",
    "\n",
    "    idx = list(range(len(dataset)))\n",
    "    optimizer = t.optim.SGD(\n",
    "        sgld_params.get_updated_model_parameters(model),\n",
    "        weight_decay=0,\n",
    "        lr=1,\n",
    "    )\n",
    "\n",
    "    submodule_param_mask = get_submodule_param_mask(\n",
    "        model, sgld_params.get_updated_model_parameters\n",
    "    ).to(device)\n",
    "\n",
    "    w_0 = (\n",
    "        t.nn.utils.parameters_to_vector(model.parameters()).detach().clone().to(device)\n",
    "    )\n",
    "\n",
    "    # Compute cross entropy loss\n",
    "    cross_entropy_loss_value = get_full_train_loss(\n",
    "        model, dataset, device, logit_scaling=sgld_params.logit_scaling\n",
    "    )\n",
    "\n",
    "    # Compute gradients using torch.autograd.grad\n",
    "    gradients = t.autograd.grad(\n",
    "        cross_entropy_loss_value, model.parameters(), create_graph=True\n",
    "    )\n",
    "    ce_loss_grad_w0 = (\n",
    "        t.nn.utils.parameters_to_vector(gradients).detach().clone().to(device)\n",
    "    )\n",
    "    ce_loss_grad_w0 *= submodule_param_mask\n",
    "    ce_loss_grad_w0 /= ce_loss_grad_w0.norm(p=2)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    array_loss = []\n",
    "    array_weight_norm = []\n",
    "    magnitude_modes = []\n",
    "\n",
    "    frame_every = sgld_params.n_steps // 50\n",
    "    sample_every = None\n",
    "    if sgld_params.num_point_samples is not None:\n",
    "        sample_every = sgld_params.n_steps // sgld_params.num_point_samples\n",
    "        # make directory for point samples\n",
    "        os.makedirs(\"point_samples\", exist_ok=True)\n",
    "        # empty directory\n",
    "        files = glob(\"point_samples/*.json\")\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "    mag_every = None\n",
    "    if sgld_params.n_magnitude_samples is not None:\n",
    "        mag_every = sgld_params.n_steps // sgld_params.n_magnitude_samples\n",
    "    step = 0\n",
    "    for sgld_step in tqdm(range(sgld_params.n_steps)):\n",
    "        batch_idx = random.choices(idx, k=sgld_params.m)\n",
    "        X1 = t.stack([dataset[b][0] for b in batch_idx]).to(device)\n",
    "        X2 = t.stack([dataset[b][1] for b in batch_idx]).to(device)\n",
    "        X3 = t.stack([dataset[b][2] for b in batch_idx]).to(device)\n",
    "        X4 = t.stack([dataset[b][3] for b in batch_idx]).to(device)\n",
    "        Y1 = t.stack([dataset[b][4] for b in batch_idx]).to(device)\n",
    "        Y2 = t.stack([dataset[b][5] for b in batch_idx]).to(device)\n",
    "        \n",
    "        seeds = t.rand(Y1.size(0), device=device)\n",
    "        mask = seeds < 0.5\n",
    "        labels = Y1.clone()\n",
    "        labels[~mask] = Y2[~mask]\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        out = model(X1, X2, X3, X4)\n",
    "        cross_entropy_loss_value = cross_entropy_loss(\n",
    "            out, labels, logit_scaling=sgld_params.logit_scaling\n",
    "        )\n",
    "        \n",
    "        array_loss.append(cross_entropy_loss_value.item())\n",
    "        w = t.nn.utils.parameters_to_vector(model.parameters())\n",
    "        array_weight_norm.append((w * submodule_param_mask).norm(p=2).item())\n",
    "        elasticity_loss_term = (sgld_params.gamma / 2) * t.sum(((w_0 - w) ** 2))\n",
    "        weight_size_term = t.sum(w**2) * (sgld_params.weight_decay / 2)\n",
    "        log_likelihood_loss_term = (\n",
    "            cross_entropy_loss_value + weight_size_term\n",
    "        ) * inverse_temp\n",
    "        full_loss = (sgld_params.epsilon / 2) * (\n",
    "            elasticity_loss_term + log_likelihood_loss_term\n",
    "        )\n",
    "        full_loss.backward()\n",
    "        optimizer.step()\n",
    "        eta = (\n",
    "            t.randn_like(w, device=device)\n",
    "            * sqrt(sgld_params.epsilon)\n",
    "            * submodule_param_mask\n",
    "        )\n",
    "        with t.no_grad():\n",
    "            new_params = t.nn.utils.parameters_to_vector(model.parameters()) + eta\n",
    "            if sgld_params.restrict_to_orth_grad:\n",
    "                diff = new_params - w_0\n",
    "                proj_diff = diff - t.dot(diff, ce_loss_grad_w0) * ce_loss_grad_w0\n",
    "                new_params = w_0 + proj_diff\n",
    "            t.nn.utils.vector_to_parameters(new_params, model.parameters())\n",
    "\n",
    "        if sgld_step % frame_every == 0 and sgld_params.movie:\n",
    "            viz_weights_modes(\n",
    "                model.embedding.weight.detach().cpu(),\n",
    "                out.shape[-1],\n",
    "                f\"frames/embeddings_movie_{step:06}.png\",\n",
    "            )\n",
    "            step += 1\n",
    "\n",
    "        if sample_every is not None:\n",
    "            if sgld_step % sample_every == 0:\n",
    "                with t.no_grad():\n",
    "                    full_loss_value = get_full_train_loss(\n",
    "                        model, dataset, device, logit_scaling=sgld_params.logit_scaling\n",
    "                    ).item()\n",
    "                    data = {\n",
    "                        \"full_loss\": float(full_loss_value),\n",
    "                        \"new_params\": list(\n",
    "                            [float(x) for x in new_params.cpu().numpy().flatten()]\n",
    "                        ),\n",
    "                    }\n",
    "                    with open(\n",
    "                        f\"point_samples/point_sample_{sgld_step:06}.json\", \"w\"\n",
    "                    ) as f:\n",
    "                        json.dump(data, f)\n",
    "        if mag_every is not None:\n",
    "            if sgld_step % mag_every == 0:\n",
    "                with t.no_grad():\n",
    "                    p = model.embedding.weight.shape[0]\n",
    "                    modes = get_magnitude_modes(\n",
    "                        model.embedding.weight.detach().cpu(), p\n",
    "                    )\n",
    "                    modes = modes.tolist()\n",
    "                    modes = modes[1 : p // 2 + 1]\n",
    "                    magnitude_modes.append(modes)\n",
    "\n",
    "    lambda_hat = (mean(array_loss[len(array_loss) // 4 :]) - init_loss) * inverse_temp\n",
    "\n",
    "    print(f\"lambda_hat: {lambda_hat}\")\n",
    "    print(f\"init_loss: {init_loss}\")\n",
    "    print(f\"sgld_params: {sgld_params}\")\n",
    "    print(f\"array_loss: {array_loss[::len(array_loss)//20]}\")\n",
    "    print(f\"array_weight_norm: {array_weight_norm[::len(array_weight_norm)//20]}\")\n",
    "    if sgld_params.movie:\n",
    "        run_movie_cmd(\"sgld\")\n",
    "    if sgld_params.num_point_samples is not None:\n",
    "        point_sample_pca(colormapping_loss=True)\n",
    "    if len(magnitude_modes) > 0:\n",
    "        # Get indices of largest 2 modes at init\n",
    "        init_modes = magnitude_modes[0]\n",
    "        init_modes = sorted(\n",
    "            range(len(init_modes)), key=lambda i: init_modes[i], reverse=True\n",
    "        )\n",
    "        mode_1 = init_modes[0]\n",
    "        mode_2 = init_modes[1]\n",
    "        # Get the values of mode_1 and mode_2 at each step\n",
    "        mode_1_values = [m[mode_1] for m in magnitude_modes]\n",
    "        mode_2_values = [m[mode_2] for m in magnitude_modes]\n",
    "        # Plot\n",
    "        plt.clf()\n",
    "        fig, ax = plt.subplots()\n",
    "        cmap = plt.cm.get_cmap(\"rainbow\")\n",
    "        norm = plt.Normalize(vmin=0, vmax=len(mode_1_values))\n",
    "        colors = [cmap(norm(i)) for i in range(len(mode_1_values))]\n",
    "        ax.scatter(mode_1_values, mode_2_values, marker=\"o\", s=10, c=colors)\n",
    "        ax.set_xlabel(f\"Mode {mode_1+1}\")\n",
    "        ax.set_ylabel(f\"Mode {mode_2+1}\")\n",
    "        fig.savefig(\n",
    "            f'plots/magnitude_modes_SGLD_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "        )\n",
    "        plt.clf()\n",
    "        fig, ax = plt.subplots()\n",
    "        for i, m in enumerate(magnitude_modes):\n",
    "            ax.plot(\n",
    "                m[: p // 2 + 1],\n",
    "                label=f\"Step {mag_every*i}\",\n",
    "                marker=\"o\",\n",
    "                c=colors[i],\n",
    "                markersize=6,\n",
    "            )\n",
    "        ax.set_xlabel(\"Mode\")\n",
    "        ax.set_ylabel(\"Magnitude\")\n",
    "        ax.legend()\n",
    "        # title\n",
    "        plt.title(\"Fourier mode magnitude vs. SGLD sampling checkpoint\")\n",
    "        fig.savefig(\n",
    "            f'plots/magnitude_modes_SGLD_all_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "        )\n",
    "    return model, lambda_hat\n",
    "\n",
    "\n",
    "def point_sample_pca(colormapping_loss=True):\n",
    "    # get point samples from /point_samples and plot pca in weight space with color corresponding to full loss (use rainbow colormap)\n",
    "    files = glob(\"point_samples/*.json\")\n",
    "    # sort filenames by sgld step\n",
    "    files = sorted(files, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    points = []\n",
    "    losses = []\n",
    "    for f in files:\n",
    "        with open(f, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            points.append(data[\"new_params\"])\n",
    "            losses.append(data[\"full_loss\"])\n",
    "    points = t.tensor(points)\n",
    "    losses = t.tensor(losses)\n",
    "    # normalize points\n",
    "    points = (points - points.mean(dim=0)) / points.std(dim=0)\n",
    "    # fillnan with 0\n",
    "    points[points != points] = 0\n",
    "\n",
    "    print(\"points\", points.shape)\n",
    "    print(\"losses\", losses.shape)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(points)\n",
    "    points_pca = pca.transform(points)\n",
    "    print(\"points_pca\", points_pca.shape)\n",
    "    print(\"explained variance\", pca.explained_variance_ratio_)\n",
    "    print(\"singular values\", pca.singular_values_)\n",
    "    print(\"components\", pca.components_)\n",
    "    print(\"mean\", pca.mean_)\n",
    "    print(\"noise variance\", pca.noise_variance_)\n",
    "    print(\"losses\", losses)\n",
    "\n",
    "    # Plot 200 equally spaced points\n",
    "    points_pca = points_pca[:: len(points_pca) // 200]\n",
    "    losses = losses[:: len(losses) // 200]\n",
    "\n",
    "    # Plot PCA\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Round losses to 2 decimal places\n",
    "    losses = [float(l) for l in losses]\n",
    "\n",
    "    # map color to loss\n",
    "    if colormapping_loss:\n",
    "        cmap = plt.cm.get_cmap(\"rainbow\")\n",
    "        norm = plt.Normalize(vmin=min(losses), vmax=max(losses))\n",
    "        colors = [cmap(norm(l)) for l in losses]\n",
    "    else:\n",
    "        cmap = plt.cm.get_cmap(\"rainbow\")\n",
    "        norm = plt.Normalize(vmin=0, vmax=len(losses))\n",
    "        colors = [cmap(norm(i)) for i in range(len(losses))]\n",
    "\n",
    "    # Plot points with small marker size\n",
    "    ax.scatter(points_pca[:, 0], points_pca[:, 1], c=colors, s=6)\n",
    "\n",
    "    # Label points with loss values\n",
    "    # for i, txt in enumerate([f\"{i}, {round(l, 2)}\" for i, l in enumerate(losses)]):\n",
    "    #     ax.annotate(txt, (points_pca[i][0], points_pca[i][1]), fontsize=7)\n",
    "\n",
    "    # Set x and y labels\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "\n",
    "    fig.savefig(\n",
    "        f'plots/point_samples_pca_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "    )\n",
    "\n",
    "\n",
    "def hyperparameter_search(\n",
    "    params_modular_addition_file,\n",
    "    params_random_file,\n",
    "    n_steps,\n",
    "    m,\n",
    "    epsilon_range,\n",
    "    gamma_range,\n",
    "):\n",
    "    sgld_params = SGLDParams()\n",
    "    sgld_params.m = m\n",
    "    params_modular_addition = ExperimentParams.load_from_file(\n",
    "        params_modular_addition_file\n",
    "    )\n",
    "    params_random = ExperimentParams.load_from_file(params_random_file)\n",
    "    random_dataset = make_random_dataset(params_random.p, params_random.random_seed)\n",
    "    modular_addition_dataset = make_dataset(params_modular_addition.p)\n",
    "    random_dataset, _ = train_test_split(\n",
    "        random_dataset, params_random.train_frac, params_random.random_seed\n",
    "    )\n",
    "    modular_addition_dataset, _ = train_test_split(\n",
    "        modular_addition_dataset,\n",
    "        params_modular_addition.train_frac,\n",
    "        params_modular_addition.random_seed,\n",
    "    )\n",
    "    results_random = defaultdict(list)\n",
    "    results_modular_addition = defaultdict(list)\n",
    "    for epsilon in epsilon_range:\n",
    "        actual_n_steps = int(n_steps / epsilon)\n",
    "        sgld_params.epsilon = epsilon\n",
    "        sgld_params.n_steps = actual_n_steps\n",
    "        for gamma in gamma_range:\n",
    "            mlp_modular_addition = MLP(params_modular_addition)\n",
    "            mlp_random = MLP(params_random)\n",
    "            sgld_params.gamma = gamma\n",
    "            mlp_modular_addition.load_state_dict(\n",
    "                t.load(f\"models/model_{params_modular_addition.get_suffix()}.pt\")\n",
    "            )\n",
    "            mlp_random.load_state_dict(\n",
    "                t.load(f\"models/model_{params_random.get_suffix()}.pt\")\n",
    "            )\n",
    "            _, lambda_hat_modular_addition = sgld(\n",
    "                mlp_modular_addition,\n",
    "                sgld_params,\n",
    "                modular_addition_dataset,\n",
    "                params_modular_addition.device,\n",
    "            )\n",
    "            _, lambda_hat_random = sgld(\n",
    "                mlp_random,\n",
    "                sgld_params,\n",
    "                random_dataset,\n",
    "                params_random.device,\n",
    "            )\n",
    "            results_modular_addition[epsilon].append(lambda_hat_modular_addition)\n",
    "            results_random[epsilon].append(lambda_hat_random)\n",
    "    # plot results\n",
    "    for epsilon in epsilon_range:\n",
    "        plt.clf()\n",
    "        plt.figure()\n",
    "        plt.plot(\n",
    "            gamma_range, results_modular_addition[epsilon], label=\"modular addition\"\n",
    "        )\n",
    "        plt.plot(gamma_range, results_random[epsilon], label=\"random\")\n",
    "        plt.title(\n",
    "            f\"$\\lambda$ vs $\\gamma$ ($\\epsilon$={epsilon}, n_steps={n_steps}, m={m})\"\n",
    "        )\n",
    "        plt.xlabel(\"$\\gamma$\")\n",
    "        plt.ylabel(\"$\\hat{\\lambda}$\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\n",
    "            f'plots/lambda_vs_gamma_epsilon_{epsilon}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "        )\n",
    "        plt.close()\n",
    "    for idx, gamma in enumerate(gamma_range):\n",
    "        plt.clf()\n",
    "        plt.figure()\n",
    "        results_per_epsilon_modular_addition = [\n",
    "            results_modular_addition[epsilon][idx] for epsilon in epsilon_range\n",
    "        ]\n",
    "        results_per_epsilon_random = [\n",
    "            results_random[epsilon][idx] for epsilon in epsilon_range\n",
    "        ]\n",
    "        plt.plot(\n",
    "            epsilon_range,\n",
    "            results_per_epsilon_modular_addition,\n",
    "            label=\"modular addition\",\n",
    "        )\n",
    "        plt.plot(epsilon_range, results_per_epsilon_random, label=\"random\")\n",
    "        plt.title(\n",
    "            f\"$\\lambda$ vs $\\epsilon$ ($\\gamma$={gamma}, n_steps={n_steps}, m={m})\"\n",
    "        )\n",
    "        plt.xlabel(\"$\\epsilon$\")\n",
    "        plt.ylabel(\"$\\hat{\\lambda}$\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\n",
    "            f'plots/lambda_vs_epsilon_gamma_{gamma}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def n_mult_search(\n",
    "    params_modular_addition_file,\n",
    "    params_random_file,\n",
    "    sgld_params,\n",
    "    n_multiplier_range,\n",
    "):\n",
    "    params_modular_addition = ExperimentParams.load_from_file(\n",
    "        params_modular_addition_file\n",
    "    )\n",
    "    params_random = ExperimentParams.load_from_file(params_random_file)\n",
    "    random_dataset = make_random_dataset(params_random.p, params_random.random_seed)\n",
    "    modular_addition_dataset = make_dataset(params_modular_addition.p)\n",
    "    random_dataset, _ = train_test_split(\n",
    "        random_dataset, params_random.train_frac, params_random.random_seed\n",
    "    )\n",
    "    modular_addition_dataset, _ = train_test_split(\n",
    "        modular_addition_dataset,\n",
    "        params_modular_addition.train_frac,\n",
    "        params_modular_addition.random_seed,\n",
    "    )\n",
    "    results_random = []\n",
    "    results_modular_addition = []\n",
    "    for n_mult in n_multiplier_range:\n",
    "        mlp_modular_addition = TwoPMLP(params_modular_addition)\n",
    "        mlp_random = TwoPMLP(params_random)\n",
    "        sgld_params.n_multiplier = n_mult\n",
    "        mlp_modular_addition.load_state_dict(\n",
    "            t.load(f\"models/model_{params_modular_addition.get_suffix()}.pt\")\n",
    "        )\n",
    "        mlp_random.load_state_dict(\n",
    "            t.load(f\"models/model_{params_random.get_suffix()}.pt\")\n",
    "        )\n",
    "        _, lambda_hat_modular_addition = sgld(\n",
    "            mlp_modular_addition,\n",
    "            sgld_params,\n",
    "            modular_addition_dataset,\n",
    "            params_modular_addition.device,\n",
    "        )\n",
    "        _, lambda_hat_random = sgld(\n",
    "            mlp_random,\n",
    "            sgld_params,\n",
    "            random_dataset,\n",
    "            params_random.device,\n",
    "        )\n",
    "        results_modular_addition.append(lambda_hat_modular_addition)\n",
    "        results_random.append(lambda_hat_random)\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    plt.plot(\n",
    "        n_multiplier_range,\n",
    "        results_modular_addition,\n",
    "        label=\"modular addition\",\n",
    "        marker=\"o\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        n_multiplier_range, results_random, label=\"random\", marker=\"o\", linestyle=\"--\"\n",
    "    )\n",
    "    plt.title(f\"$\\hat\\lambda$ vs n_mult\")\n",
    "    plt.xlabel(\"n_mult\")\n",
    "    plt.ylabel(\"$\\hat{\\lambda}$\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\n",
    "        f'plots/lambda_vs_n_mult_{n_mult}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def get_lambda(params, sgld_params, checkpoint_no=None):\n",
    "    model = TwoPMLP(params)\n",
    "    if checkpoint_no is None:\n",
    "        model.load_state_dict(t.load(f\"models/model_{params.get_suffix()}.pt\"))\n",
    "    else:\n",
    "        model.load_state_dict(\n",
    "            t.load(\n",
    "                f\"models/checkpoints/{params.get_suffix(checkpoint_no=checkpoint_no)}.pt\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    dataset = make_two_p_dataset(params)\n",
    "    train_data, test_data = train_test_split(\n",
    "        dataset, params\n",
    "    )\n",
    "    test_loss = get_full_train_loss(model, test_data, params.device)\n",
    "    train_loss = get_full_train_loss(model, train_data, params.device)\n",
    "    _, lambda_hat = sgld(model, sgld_params, train_data, params.device)\n",
    "    lambda_hat, test_loss, train_loss = lambda_hat.cpu().item(), test_loss.cpu().item(), train_loss.cpu().item()\n",
    "    return lambda_hat, test_loss, train_loss\n",
    "\n",
    "\n",
    "def get_lambda_per_quantity(param_files, sgld_params, resample=True):\n",
    "    lambda_values = []\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    for param_file in param_files:\n",
    "        params = ExperimentParams.load_from_file(param_file)\n",
    "        if not resample and params.lambda_hat is not None:\n",
    "            lambda_values.append(params.lambda_hat)\n",
    "            continue\n",
    "        lambda_hat, test_loss, train_loss = get_lambda(params, sgld_params)\n",
    "        lambda_values.append(lambda_hat)\n",
    "        test_losses.append(test_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        param_dict = params.get_dict()\n",
    "        param_dict[\"lambda_hat\"] = lambda_hat\n",
    "        param_dict[\"test_loss\"] = test_loss\n",
    "        param_dict[\"train_loss\"] = train_loss\n",
    "        with open(param_file, \"w\") as f:\n",
    "            json.dump(param_dict, f)\n",
    "    return lambda_values, test_losses, train_losses\n",
    "\n",
    "\n",
    "def plot_lambda_test_train_loss(\n",
    "    ax1, x_axis, x_label, lambda_values, test_losses, train_losses\n",
    "):\n",
    "    # Plot lambda values on the left y-axis\n",
    "    ax1.plot(x_axis, lambda_values, marker=\"o\", label=\"$\\hat{\\lambda}$\", color=\"g\")\n",
    "    # ax1.plot(x_axis, [8 * x for x in x_axis], label=\"y=8x\", linestyle=\"--\")\n",
    "    ax1.set_xlabel(x_label)\n",
    "    ax1.set_ylabel(\"$\\hat{\\lambda}$\")\n",
    "    ax1.tick_params(\"y\", colors=\"g\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "\n",
    "    # Create a second y-axis for the losses\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(\n",
    "        x_axis, train_losses, marker=\"o\", color=\"b\", label=\"train loss\", linestyle=\"--\"\n",
    "    )\n",
    "    ax2.plot(\n",
    "        x_axis, test_losses, marker=\"o\", color=\"r\", label=\"test loss\", linestyle=\"--\"\n",
    "    )\n",
    "    ax2.set_ylabel(\"Loss\", color=\"b\")\n",
    "    ax2.tick_params(\"y\", colors=\"b\")\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "def plot_lambda_per_quantity(param_files, quantity_values, quantity_name, sgld_params):\n",
    "    lambda_values, test_losses, train_losses = get_lambda_per_quantity(\n",
    "        param_files, sgld_params\n",
    "    )\n",
    "\n",
    "    # Clear previous plots\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    plot_lambda_test_train_loss(\n",
    "        ax1, quantity_values, quantity_name, lambda_values, test_losses, train_losses\n",
    "    )\n",
    "\n",
    "    # Set title\n",
    "    ax1.set_title(f\"$\\lambda$ vs {quantity_name}\")\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(\n",
    "        f'plots/lambda_vs_{quantity_name.replace(\" \", \"_\")}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "    )\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_lambda_per_checkpoint(param_file, sgld_params, checkpoints=None):\n",
    "    lambda_values = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    params = ExperimentParams.load_from_file(param_file)\n",
    "    check_list = list(range(params.n_save_model_checkpoints))\n",
    "    if checkpoints is not None:\n",
    "        check_list = checkpoints\n",
    "    for i in check_list:\n",
    "        lambda_hat, test_loss, train_loss = get_lambda(\n",
    "            params, sgld_params, checkpoint_no=i\n",
    "        )\n",
    "        lambda_values.append(lambda_hat)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    # Clear previous plots\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot lambda values on the left y-axis\n",
    "    plot_lambda_test_train_loss(\n",
    "        ax1, check_list, \"Checkpoint\", lambda_values, test_losses, train_losses\n",
    "    )\n",
    "\n",
    "    # Set title\n",
    "    title = \"$\\lambda$ vs checkpoint\"\n",
    "    if sgld_params.restrict_to_orth_grad:\n",
    "        title += \" (restrict orth dir)\"\n",
    "    if sgld_params.n_multiplier != 1:\n",
    "        title += f\" (n*={sgld_params.n_multiplier})\"\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(\n",
    "        f'plots/lambda_vs_checkpoint_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}_restrictorth{sgld_params.restrict_to_orth_grad}.png'\n",
    "    )\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_lambda_per_checkpoint_multi_run(sgld_params, sweep_dir):\n",
    "    files = glob(f\"{sweep_dir}/*.json\")\n",
    "\n",
    "    results = defaultdict(dict)\n",
    "    for f in files:\n",
    "        exp_params = ExperimentParams.load_from_file(f)\n",
    "        run_id = exp_params.run_id\n",
    "        n_checkpoints = exp_params.n_save_model_checkpoints\n",
    "        check_list = list(range(n_checkpoints))\n",
    "        for c in check_list:\n",
    "            lambda_hat, test_loss, train_loss = get_lambda(\n",
    "                exp_params, sgld_params, checkpoint_no=c\n",
    "            )\n",
    "            results[run_id][c] = (lambda_hat, test_loss, train_loss)\n",
    "\n",
    "    # Clear previous plots\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    # Plot mean lambda, test loss, and train loss per checkpoint (mean over runs)\n",
    "    run_ids = sorted(list(results.keys()))\n",
    "    lambda_values = []\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    for c in check_list:\n",
    "        lambda_values.append(t.mean(t.tensor([run[c][0] for run in results.values()])))\n",
    "        test_losses.append(t.mean(t.tensor([run[c][1] for run in results.values()])))\n",
    "        train_losses.append(t.mean(t.tensor([run[c][2] for run in results.values()])))\n",
    "    plot_lambda_test_train_loss(\n",
    "        ax1, check_list, \"Checkpoint\", lambda_values, test_losses, train_losses\n",
    "    )\n",
    "\n",
    "    # Set title\n",
    "    title = \"$\\lambda$ vs checkpoint\"\n",
    "    if sgld_params.restrict_to_orth_grad:\n",
    "        title += \" (restrict orth dir)\"\n",
    "    if sgld_params.n_multiplier != 1:\n",
    "        title += f\" (n*={sgld_params.n_multiplier})\"\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(\n",
    "        f'plots/lambda_vs_checkpoint_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}_restrictorth{sgld_params.restrict_to_orth_grad}_meanover_{len(run_ids)}.png'\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_lambda_per_p(sgld_params, p_sweep_dir, resample=False, append_to_title=\"\"):\n",
    "    files = glob(f\"{p_sweep_dir}/*.json\")\n",
    "    results = defaultdict(dict)\n",
    "    for f in files:\n",
    "        exp_params = ExperimentParams.load_from_file(f)\n",
    "        p = exp_params.p\n",
    "        run_id = exp_params.run_id\n",
    "        if not resample and all(\n",
    "            [\n",
    "                exp_params.lambda_hat is not None,\n",
    "                exp_params.test_loss is not None,\n",
    "                exp_params.train_loss is not None,\n",
    "            ]\n",
    "        ):\n",
    "            results[p][run_id] = (\n",
    "                exp_params.lambda_hat,\n",
    "                exp_params.test_loss,\n",
    "                exp_params.train_loss,\n",
    "            )\n",
    "            continue\n",
    "        lambda_hat, test_loss, train_loss = get_lambda(exp_params, sgld_params)\n",
    "        results[p][run_id] = (lambda_hat, test_loss, train_loss)\n",
    "        exp_params.lambda_hat = lambda_hat\n",
    "        exp_params.test_loss = test_loss\n",
    "        exp_params.train_loss = train_loss\n",
    "        exp_params.save_to_file(f)\n",
    "    print(\"Extracted results\", results)\n",
    "\n",
    "    # Plot average lambda, test loss, and train loss per p (mean over runs)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    p_values = sorted(list(results.keys()))\n",
    "    lambda_values = []\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    for p in p_values:\n",
    "        lambda_values.append(t.mean(t.tensor([run[0] for run in results[p].values()])))\n",
    "        test_losses.append(t.mean(t.tensor([run[1] for run in results[p].values()])))\n",
    "        train_losses.append(t.mean(t.tensor([run[2] for run in results[p].values()])))\n",
    "    plot_lambda_test_train_loss(\n",
    "        ax1, p_values, \"p\", lambda_values, test_losses, train_losses\n",
    "    )\n",
    "    ax1.set_title(\"$\\lambda$ vs p\")\n",
    "    fig.savefig(\n",
    "        f'plots/lambda_vs_p_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}_{append_to_title}.png'\n",
    "    )\n",
    "    print(\"Saved plot 1\")\n",
    "\n",
    "    # Plot lambda (each run is a different color)\n",
    "    run_ids = sorted(list(results[p_values[0]].keys()))\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    for run_id in run_ids:\n",
    "        lambda_values = []\n",
    "        for p in p_values:\n",
    "            lambda_values.append(results[p][run_id][0])\n",
    "        ax1.plot(p_values, lambda_values, marker=\"o\", label=f\"Run {run_id}\")\n",
    "    ax1.set_xlabel(\"p\")\n",
    "    ax1.set_ylabel(\"$\\hat{\\lambda}$\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.set_title(\"$\\lambda$ vs p\")\n",
    "    fig.savefig(\n",
    "        f'plots/lambda_vs_p_runs_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "    )\n",
    "    print(\"Saved plot 2\")\n",
    "\n",
    "\n",
    "def plot_lambda_per_p_different_exps(exp_dirs, exp_names, sgld_params, resample=False):\n",
    "    \"\"\"\n",
    "    exp_dirs: list of directories containing experiment params json files corresponding to different p value sweeps\n",
    "    exp_names: name of experiment being run for that p sweep\n",
    "    sgld_params: List of SGLDParams objects, or single settings - SGLD settings to use for each exp_dir\n",
    "    resample: whether to resample if lambda already saved\n",
    "    \"\"\"\n",
    "    # If sgld_params is not a list, make it a list\n",
    "    if not isinstance(sgld_params, list):\n",
    "        sgld_params = [sgld_params] * len(exp_dirs)\n",
    "    results = defaultdict(lambda: defaultdict(list))\n",
    "    for i, d in enumerate(exp_dirs):\n",
    "        files = glob(f\"{d}/*.json\")\n",
    "        for f in files:\n",
    "            exp_params = ExperimentParams.load_from_file(f)\n",
    "            p = exp_params.p\n",
    "            if not resample and exp_params.lambda_hat is not None:\n",
    "                results[exp_names[i]][p].append(exp_params.lambda_hat)\n",
    "            else:\n",
    "                lambda_hat, _, _ = get_lambda(exp_params, sgld_params[i])\n",
    "                results[exp_names[i]][p].append(lambda_hat)\n",
    "                exp_params.lambda_hat = lambda_hat\n",
    "                exp_params.save_to_file(f)\n",
    "\n",
    "    # lambda per p for each exp on same plot\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    for n in exp_names:\n",
    "        p_values = sorted(list(results[n].keys()))\n",
    "        lambda_values = []\n",
    "        for p in p_values:\n",
    "            l = sum(results[n][p]) / len(results[n][p])\n",
    "            lambda_values.append(l)\n",
    "        plt.plot(p_values, lambda_values, marker=\"o\", label=n, linestyle=\"--\")\n",
    "    plt.xlabel(\"p\")\n",
    "    plt.ylabel(\"$\\hat{\\lambda}$\")\n",
    "    plt.legend()\n",
    "    plt.title(\"$\\lambda$ vs p\")\n",
    "    fig.savefig(f'plots/lambda_vs_p_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png')\n",
    "\n",
    "\n",
    "def plot_lambda_per_frac(sgld_params, frac_sweep_dir, resample=False):\n",
    "    files = glob(f\"{frac_sweep_dir}/*.json\")\n",
    "    results = defaultdict(dict)\n",
    "    for f in files:\n",
    "        exp_params = ExperimentParams.load_from_file(f)\n",
    "        frac = exp_params.train_frac\n",
    "        run_id = exp_params.run_id\n",
    "        if not resample and all(\n",
    "            [\n",
    "                exp_params.lambda_hat is not None,\n",
    "                exp_params.test_loss is not None,\n",
    "                exp_params.train_loss is not None,\n",
    "            ]\n",
    "        ):\n",
    "            results[frac][run_id] = (\n",
    "                exp_params.lambda_hat,\n",
    "                exp_params.test_loss,\n",
    "                exp_params.train_loss,\n",
    "            )\n",
    "            continue\n",
    "        lambda_hat, test_loss, train_loss = get_lambda(exp_params, sgld_params)\n",
    "        results[frac][run_id] = (lambda_hat, test_loss, train_loss)\n",
    "        exp_params.lambda_hat = lambda_hat\n",
    "        exp_params.test_loss = test_loss\n",
    "        exp_params.train_loss = train_loss\n",
    "        exp_params.save_to_file(f)\n",
    "    print(\"Extracted results\", results)\n",
    "\n",
    "    # Plot average lambda, test loss, and train loss per p (mean over runs)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    frac_values = sorted(list(results.keys()))\n",
    "    lambda_values = []\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    for frac in frac_values:\n",
    "        lambda_values.append(\n",
    "            t.mean(t.tensor([run[0] for run in results[frac].values()]))\n",
    "        )\n",
    "        test_losses.append(t.mean(t.tensor([run[1] for run in results[frac].values()])))\n",
    "        train_losses.append(\n",
    "            t.mean(t.tensor([run[2] for run in results[frac].values()]))\n",
    "        )\n",
    "    plot_lambda_test_train_loss(\n",
    "        ax1, frac_values, \"train_frac\", lambda_values, test_losses, train_losses\n",
    "    )\n",
    "    ax1.set_title(\"$\\lambda$ vs train_frac\")\n",
    "    fig.savefig(f'plots/lambda_vs_frac_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png')\n",
    "\n",
    "\n",
    "def slope_vs_n_mult(sgld_params, exp_dir, n_mults):\n",
    "    files = glob(f\"{exp_dir}/*.json\")\n",
    "    slopes = []\n",
    "    for n_mult in n_mults:\n",
    "        sgld_params.n_multiplier = n_mult\n",
    "        results = defaultdict(dict)\n",
    "        for f in files:\n",
    "            exp_params = ExperimentParams.load_from_file(f)\n",
    "            p = exp_params.p\n",
    "            run_id = exp_params.run_id\n",
    "            lambda_hat, test_loss, train_loss = get_lambda(exp_params, sgld_params)\n",
    "            results[p][run_id] = (\n",
    "                lambda_hat,\n",
    "                test_loss,\n",
    "                train_loss,\n",
    "            )\n",
    "            exp_params.lambda_hat = lambda_hat\n",
    "            exp_params.test_loss = test_loss\n",
    "            exp_params.train_loss = train_loss\n",
    "            exp_params.save_to_file(f)\n",
    "\n",
    "        with open(f\"results/results_n_mult_{n_mult}.json\", \"w\") as f:\n",
    "            json.dump(dict(results), f)\n",
    "\n",
    "        avg_over_runs = defaultdict(list)\n",
    "        for p in results.keys():\n",
    "            for run_id in results[p].keys():\n",
    "                avg_over_runs[p].append(results[p][run_id][0])\n",
    "        for p in avg_over_runs.keys():\n",
    "            avg_over_runs[p] = sum(avg_over_runs[p]) / len(avg_over_runs[p])\n",
    "        p1 = list(avg_over_runs.keys())[0]\n",
    "        p2 = list(avg_over_runs.keys())[-1]\n",
    "        avg_lambda_p1 = avg_over_runs[p1]\n",
    "        avg_lambda_p2 = avg_over_runs[p2]\n",
    "        slope = abs((avg_lambda_p2 - avg_lambda_p1) / (p2 - p1))\n",
    "        slopes.append(slope)\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    plt.plot(n_mults, slopes, marker=\"o\")\n",
    "    plt.title(\"Slope of $\\lambda$ vs p\")\n",
    "    plt.xlabel(\"n_multiplier\")\n",
    "    plt.ylabel(\"Slope\")\n",
    "    # log scale on x axis\n",
    "    plt.xscale(\"log\")\n",
    "    plt.savefig(\n",
    "        f'plots/slope_lambda_vs_p_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def grok_exp():\n",
    "    temp_multipliers = [0.1, 0.3, 1, 3]\n",
    "    files = {\n",
    "        2: list(glob(\"exp_params/QUADRATIC/*2grok.json\")),\n",
    "        3: list(glob(\"exp_params/QUADRATIC/*3grok.json\")),\n",
    "        4: list(glob(\"exp_params/QUADRATIC/*4grok.json\")),\n",
    "        # 5: list(glob(\"exp_params/rerun_exps/*5grok.json\")),\n",
    "    }\n",
    "    full_results = []\n",
    "    plt.clf()\n",
    "    for tm in temp_multipliers:\n",
    "        sgld_params = SGLDParams(\n",
    "            gamma=5,\n",
    "            epsilon=0.0001,\n",
    "            n_steps=5000,\n",
    "            m=200,\n",
    "            temp_multiplier=tm,\n",
    "            get_updated_model_parameters=lambda m: m.embedding.parameters(),\n",
    "        )\n",
    "        results = []\n",
    "        for n_groks, filenames in files.items():\n",
    "            lambdas = []\n",
    "            for f in filenames:\n",
    "                exp_params = ExperimentParams.load_from_file(f)\n",
    "                lambda_hat, _, _ = get_lambda(exp_params, sgld_params)\n",
    "                lambdas.append(lambda_hat)\n",
    "            mean_lambda = sum(lambdas) / len(lambdas)\n",
    "            results.append(mean_lambda)\n",
    "        full_results.append(results)\n",
    "        plt.plot(\n",
    "            list(files.keys()),\n",
    "            results,\n",
    "            marker=\"o\",\n",
    "            label=f\"temp_multiplier={tm}\",\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "        print(f\"temp_multiplier={tm}, results={results}\")\n",
    "    print(f\"full_results={full_results}\")\n",
    "    plt.xlabel(\"# circuits\")\n",
    "    plt.ylabel(\"$\\hat{\\lambda}$\")\n",
    "    plt.xticks(list(files.keys()))\n",
    "    plt.legend()\n",
    "    plt.title(\"$\\hat{\\lambda}$ vs # circuits at different temperatures\")\n",
    "    plt.savefig(\n",
    "        f'plots/lambda_vs_n_groks_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import csv\n",
    "    import plotly.express as px\n",
    "    import pandas as pd\n",
    "    import sympy\n",
    "\n",
    "    sgld_params = SGLDParams(\n",
    "        gamma=5,\n",
    "        epsilon=0.0001,\n",
    "        n_steps=50,\n",
    "        m=64,\n",
    "        restrict_to_orth_grad=True,\n",
    "        weight_decay=0.0\n",
    "    )\n",
    "\n",
    "    primes_under_100 = sympy.primerange(1, 100)\n",
    "    llc_results = []\n",
    "    # print(list(primes_under_100))\n",
    "\n",
    "    for p1 in sympy.primerange(1, 100):\n",
    "        for p2 in sympy.primerange(1, 100):\n",
    "            print(p1, p2)\n",
    "            param_file = f\"exp_params/two_p_modadd/p1={p1}_p2={p2}.json\"\n",
    "            try:\n",
    "                params = ExperimentParamsTwoP.load_from_file(param_file)\n",
    "                check_list = list(range(params.n_save_model_checkpoints))\n",
    "                lambda_hat, test_loss, train_loss = get_lambda(params, sgld_params, check_list[-1])\n",
    "                llc_results.append({'p1': p1, 'p2': p2, 'LLC': lambda_hat})\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {param_file}\")\n",
    "                continue\n",
    "\n",
    "    with open('llc_results.csv', mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['p1', 'p2', 'LLC'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(llc_results)\n",
    "\n",
    "    df = pd.DataFrame(llc_results)\n",
    "    fig = px.scatter(df, x='p1', y='p2', color='LLC', color_continuous_scale='Viridis', symbol='LLC')\n",
    "    fig.update_traces(marker=dict(size=12))\n",
    "    fig.update_layout(title='Discrete Tiles of LLC values', xaxis_title='p1', yaxis_title='p2')\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/qfsrq8wj53d1n4vbrb800gx40000gq/T/ipykernel_18415/3763930752.py:9: UserWarning:\n",
      "\n",
      "Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "\n",
      "/var/folders/rb/qfsrq8wj53d1n4vbrb800gx40000gq/T/ipykernel_18415/3763930752.py:9: UserWarning:\n",
      "\n",
      "Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('llc_results.csv')\n",
    "\n",
    "pivot_table = df.pivot(index='p1', columns='p2', values='LLC')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(pivot_table, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='LLC')\n",
    "plt.title('Discrete Tiles of LLC values')\n",
    "plt.xlabel('p2')\n",
    "plt.ylabel('p1')\n",
    "plt.xticks(ticks=range(len(pivot_table.columns)), labels=pivot_table.columns)\n",
    "plt.yticks(ticks=range(len(pivot_table.index)), labels=pivot_table.index)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "df-stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
