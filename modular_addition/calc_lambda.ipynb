{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/qfsrq8wj53d1n4vbrb800gx40000gq/T/ipykernel_96819/4080863076.py:513: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  t.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rands:  [(3, 11), (2, 8), (5, 3), (8, 6), (3, 1)]\n",
      "Using exceptions:  False len =  20449\n",
      "First indices of shuffled dataset [9568, 9582, 9428, 19017, 19030, 16365, 5011, 14175, 8050, 17858]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:09<00:00, 211.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_hat: 2199.612548828125\n",
      "init_loss: 0.30333641171455383\n",
      "sgld_params: SGLDParams(gamma=5, epsilon=0.0001, n_steps=2000, m=64, restrict_to_orth_grad=True, get_updated_model_parameters=<function SGLDParams.<lambda> at 0x177911e40>, n_multiplier=1, movie=False, num_point_samples=None, n_magnitude_samples=None, weight_decay=0.0, logit_scaling=1.0, temp_multiplier=1.0)\n",
      "array_loss: [0.31114792823791504, 1.2372066974639893, 1.304248332977295, 1.532181978225708, 0.9714391827583313, 1.3749198913574219, 1.177415132522583, 1.3139872550964355, 1.2913686037063599, 1.9094716310501099, 1.6506775617599487, 1.0090503692626953, 1.6365424394607544, 1.3552629947662354, 1.6043298244476318, 1.9089891910552979, 1.3347280025482178, 1.6303555965423584, 1.5792698860168457, 1.589426040649414]\n",
      "array_weight_norm: [83.36106872558594, 83.64395904541016, 84.10662078857422, 84.46417236328125, 84.70330810546875, 84.96744537353516, 85.0579833984375, 85.18802642822266, 85.17052459716797, 85.32987976074219, 85.57235717773438, 85.71402740478516, 85.97510528564453, 85.9803237915039, 86.13327026367188, 86.2739486694336, 86.28286743164062, 86.49639129638672, 86.55286407470703, 86.70567321777344]\n",
      "Rands:  [(3, 11), (2, 8), (5, 3), (8, 6), (3, 1)]\n",
      "using exceptions\n",
      "Using exceptions:  True len =  20449\n",
      "First indices of shuffled dataset [9568, 9582, 9428, 19017, 19030, 16365, 5011, 14175, 8050, 17858]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:09<00:00, 219.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_hat: 1814.4642333984375\n",
      "init_loss: 1.864658236503601\n",
      "sgld_params: SGLDParams(gamma=5, epsilon=0.0001, n_steps=2000, m=64, restrict_to_orth_grad=True, get_updated_model_parameters=<function SGLDParams.<lambda> at 0x177911e40>, n_multiplier=1, movie=False, num_point_samples=None, n_magnitude_samples=None, weight_decay=0.0, logit_scaling=1.0, temp_multiplier=1.0)\n",
      "array_loss: [1.445336937904358, 2.607853889465332, 3.2039284706115723, 3.0344810485839844, 3.224809408187866, 3.211571216583252, 2.8269150257110596, 2.579531192779541, 3.5114054679870605, 2.5211424827575684, 2.836092948913574, 3.1999988555908203, 2.4393904209136963, 2.411726713180542, 2.9764626026153564, 2.4020957946777344, 2.7546842098236084, 2.67232084274292, 2.347360134124756, 2.883021593093872]\n",
      "array_weight_norm: [80.94731140136719, 81.31446075439453, 81.34146118164062, 81.29679870605469, 81.54376220703125, 81.76947784423828, 81.79570007324219, 81.91239166259766, 82.10167694091797, 81.94338989257812, 82.05694580078125, 82.2479248046875, 82.14456939697266, 82.27909851074219, 82.4876937866211, 82.68073272705078, 82.80043029785156, 82.92129516601562, 83.12702178955078, 83.30320739746094]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Exceptions False Lhat: 2199.612548828125\n",
      "Exceptions True Lhat: 1814.4642333984375\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from helpers import get_submodule_param_mask\n",
    "from math import log, sqrt\n",
    "from train import ExperimentParams\n",
    "from model import MLP\n",
    "from dataset import make_dataset, train_test_split, make_random_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Optional\n",
    "import json\n",
    "from glob import glob\n",
    "from model_viz import viz_weights_modes\n",
    "from movie import run_movie_cmd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from dynamics import get_magnitude_modes\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SGLDParams:\n",
    "    gamma: float = 1\n",
    "    epsilon: float = 0.001\n",
    "    n_steps: int = 10000\n",
    "    m: int = 512  # SGLD batch size\n",
    "    restrict_to_orth_grad: bool = False\n",
    "    get_updated_model_parameters: Callable = lambda model: model.parameters()  # Override to only search parameter subspace\n",
    "    n_multiplier: float = 1\n",
    "    movie: bool = False\n",
    "    num_point_samples: Optional[int] = None\n",
    "    n_magnitude_samples: Optional[int] = None\n",
    "    weight_decay: float = 0\n",
    "    logit_scaling: float = 1.0\n",
    "    temp_multiplier: float = 1.0\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, y_s, logit_scaling=1):\n",
    "    \"\"\"\n",
    "    logits: outputs of model\n",
    "    y: target labels\n",
    "\n",
    "    returns: mean cross entropy loss\n",
    "    \"\"\"\n",
    "    preds = t.nn.functional.softmax(logits * logit_scaling, dim=1)\n",
    "    return -1 * t.mean(t.log(preds[t.arange(len(preds)), y_s] + 1e-7))\n",
    "\n",
    "\n",
    "# def cross_entropy_loss(logits, y_s, logit_scaling=1):\n",
    "#     \"\"\"\n",
    "#     logits: outputs of model\n",
    "#     y: target labels\n",
    "\n",
    "#     returns: mean cross entropy loss\n",
    "#     \"\"\"\n",
    "#     logit_diff = logits - logits[t.arange(len(logits)), y_s].unsqueeze(1)\n",
    "#     return t.mean(t.logsumexp(logit_diff * logit_scaling, dim=1))\n",
    "\n",
    "\n",
    "def get_full_train_loss(model, dataset, device, logit_scaling=1):\n",
    "    model = model.to(device)\n",
    "    X_1 = t.stack([dataset[b][0][0] for b in range(len(dataset))]).to(device)\n",
    "    X_2 = t.stack([dataset[b][0][1] for b in range(len(dataset))]).to(device)\n",
    "    Y = t.stack([dataset[b][1] for b in range(len(dataset))]).to(device)\n",
    "    out = model(X_1, X_2)\n",
    "    return cross_entropy_loss(out, Y, logit_scaling=logit_scaling)\n",
    "\n",
    "\n",
    "def mean(arr):\n",
    "    return sum(arr) / len(arr)\n",
    "\n",
    "\n",
    "def sgld(model, sgld_params, dataset, device):\n",
    "    \"\"\"\n",
    "    model: MLP model\n",
    "    sgld_params: SGLDParams object\n",
    "    dataset: dataset to train on\n",
    "    device: device to run on\n",
    "\n",
    "    returns: updated model, lambda_hat\n",
    "    \"\"\"\n",
    "    n = len(dataset)\n",
    "    model = model.to(device)\n",
    "    effective_n = n * sgld_params.n_multiplier\n",
    "\n",
    "    inverse_temp = effective_n / log(effective_n)\n",
    "    inverse_temp /= sgld_params.temp_multiplier\n",
    "\n",
    "    init_loss = get_full_train_loss(\n",
    "        model, dataset, device, logit_scaling=sgld_params.logit_scaling\n",
    "    )\n",
    "\n",
    "    idx = list(range(len(dataset)))\n",
    "    optimizer = t.optim.SGD(\n",
    "        sgld_params.get_updated_model_parameters(model),\n",
    "        weight_decay=0,\n",
    "        lr=1,\n",
    "    )\n",
    "\n",
    "    submodule_param_mask = get_submodule_param_mask(\n",
    "        model, sgld_params.get_updated_model_parameters\n",
    "    ).to(device)\n",
    "\n",
    "    w_0 = (\n",
    "        t.nn.utils.parameters_to_vector(model.parameters()).detach().clone().to(device)\n",
    "    )\n",
    "\n",
    "    # Compute cross entropy loss\n",
    "    cross_entropy_loss_value = get_full_train_loss(\n",
    "        model, dataset, device, logit_scaling=sgld_params.logit_scaling\n",
    "    )\n",
    "\n",
    "    # Compute gradients using torch.autograd.grad\n",
    "    gradients = t.autograd.grad(\n",
    "        cross_entropy_loss_value, model.parameters(), create_graph=True\n",
    "    )\n",
    "    ce_loss_grad_w0 = (\n",
    "        t.nn.utils.parameters_to_vector(gradients).detach().clone().to(device)\n",
    "    )\n",
    "    ce_loss_grad_w0 *= submodule_param_mask\n",
    "    ce_loss_grad_w0 /= ce_loss_grad_w0.norm(p=2)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    array_loss = []\n",
    "    array_weight_norm = []\n",
    "    magnitude_modes = []\n",
    "\n",
    "    frame_every = sgld_params.n_steps // 50\n",
    "    sample_every = None\n",
    "    if sgld_params.num_point_samples is not None:\n",
    "        sample_every = sgld_params.n_steps // sgld_params.num_point_samples\n",
    "        # make directory for point samples\n",
    "        os.makedirs(\"point_samples\", exist_ok=True)\n",
    "        # empty directory\n",
    "        files = glob(\"point_samples/*.json\")\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "    mag_every = None\n",
    "    if sgld_params.n_magnitude_samples is not None:\n",
    "        mag_every = sgld_params.n_steps // sgld_params.n_magnitude_samples\n",
    "    step = 0\n",
    "    for sgld_step in tqdm(range(sgld_params.n_steps)):\n",
    "        batch_idx = random.choices(idx, k=sgld_params.m)\n",
    "        X_1 = t.stack([dataset[b][0][0] for b in batch_idx]).to(device)\n",
    "        X_2 = t.stack([dataset[b][0][1] for b in batch_idx]).to(device)\n",
    "        Y = t.stack([dataset[b][1] for b in batch_idx]).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X_1, X_2)\n",
    "        cross_entropy_loss_value = cross_entropy_loss(\n",
    "            out, Y, logit_scaling=sgld_params.logit_scaling\n",
    "        )\n",
    "        array_loss.append(cross_entropy_loss_value.item())\n",
    "        w = t.nn.utils.parameters_to_vector(model.parameters())\n",
    "        array_weight_norm.append((w * submodule_param_mask).norm(p=2).item())\n",
    "        elasticity_loss_term = (sgld_params.gamma / 2) * t.sum(((w_0 - w) ** 2))\n",
    "        weight_size_term = t.sum(w**2) * (sgld_params.weight_decay / 2)\n",
    "        log_likelihood_loss_term = (\n",
    "            cross_entropy_loss_value + weight_size_term\n",
    "        ) * inverse_temp\n",
    "        full_loss = (sgld_params.epsilon / 2) * (\n",
    "            elasticity_loss_term + log_likelihood_loss_term\n",
    "        )\n",
    "        full_loss.backward()\n",
    "        optimizer.step()\n",
    "        eta = (\n",
    "            t.randn_like(w, device=device)\n",
    "            * sqrt(sgld_params.epsilon)\n",
    "            * submodule_param_mask\n",
    "        )\n",
    "        with t.no_grad():\n",
    "            new_params = t.nn.utils.parameters_to_vector(model.parameters()) + eta\n",
    "            if sgld_params.restrict_to_orth_grad:\n",
    "                diff = new_params - w_0\n",
    "                proj_diff = diff - t.dot(diff, ce_loss_grad_w0) * ce_loss_grad_w0\n",
    "                new_params = w_0 + proj_diff\n",
    "            t.nn.utils.vector_to_parameters(new_params, model.parameters())\n",
    "\n",
    "        if sgld_step % frame_every == 0 and sgld_params.movie:\n",
    "            viz_weights_modes(\n",
    "                model.embedding.weight.detach().cpu(),\n",
    "                out.shape[-1],\n",
    "                f\"frames/embeddings_movie_{step:06}.png\",\n",
    "            )\n",
    "            step += 1\n",
    "\n",
    "        if sample_every is not None:\n",
    "            if sgld_step % sample_every == 0:\n",
    "                with t.no_grad():\n",
    "                    full_loss_value = get_full_train_loss(\n",
    "                        model, dataset, device, logit_scaling=sgld_params.logit_scaling\n",
    "                    ).item()\n",
    "                    data = {\n",
    "                        \"full_loss\": float(full_loss_value),\n",
    "                        \"new_params\": list(\n",
    "                            [float(x) for x in new_params.cpu().numpy().flatten()]\n",
    "                        ),\n",
    "                    }\n",
    "                    with open(\n",
    "                        f\"point_samples/point_sample_{sgld_step:06}.json\", \"w\"\n",
    "                    ) as f:\n",
    "                        json.dump(data, f)\n",
    "        if mag_every is not None:\n",
    "            if sgld_step % mag_every == 0:\n",
    "                with t.no_grad():\n",
    "                    p = model.embedding.weight.shape[0]\n",
    "                    modes = get_magnitude_modes(\n",
    "                        model.embedding.weight.detach().cpu(), p\n",
    "                    )\n",
    "                    modes = modes.tolist()\n",
    "                    modes = modes[1 : p // 2 + 1]\n",
    "                    magnitude_modes.append(modes)\n",
    "\n",
    "    lambda_hat = (mean(array_loss[len(array_loss) // 4 :]) - init_loss) * inverse_temp\n",
    "\n",
    "    print(f\"lambda_hat: {lambda_hat}\")\n",
    "    print(f\"init_loss: {init_loss}\")\n",
    "    print(f\"sgld_params: {sgld_params}\")\n",
    "    print(f\"array_loss: {array_loss[::len(array_loss)//20]}\")\n",
    "    print(f\"array_weight_norm: {array_weight_norm[::len(array_weight_norm)//20]}\")\n",
    "    if sgld_params.movie:\n",
    "        run_movie_cmd(\"sgld\")\n",
    "    if sgld_params.num_point_samples is not None:\n",
    "        point_sample_pca(colormapping_loss=True)\n",
    "    if len(magnitude_modes) > 0:\n",
    "        # Get indices of largest 2 modes at init\n",
    "        init_modes = magnitude_modes[0]\n",
    "        init_modes = sorted(\n",
    "            range(len(init_modes)), key=lambda i: init_modes[i], reverse=True\n",
    "        )\n",
    "        mode_1 = init_modes[0]\n",
    "        mode_2 = init_modes[1]\n",
    "        # Get the values of mode_1 and mode_2 at each step\n",
    "        mode_1_values = [m[mode_1] for m in magnitude_modes]\n",
    "        mode_2_values = [m[mode_2] for m in magnitude_modes]\n",
    "        # Plot\n",
    "        plt.clf()\n",
    "        fig, ax = plt.subplots()\n",
    "        cmap = plt.cm.get_cmap(\"rainbow\")\n",
    "        norm = plt.Normalize(vmin=0, vmax=len(mode_1_values))\n",
    "        colors = [cmap(norm(i)) for i in range(len(mode_1_values))]\n",
    "        ax.scatter(mode_1_values, mode_2_values, marker=\"o\", s=10, c=colors)\n",
    "        ax.set_xlabel(f\"Mode {mode_1+1}\")\n",
    "        ax.set_ylabel(f\"Mode {mode_2+1}\")\n",
    "        fig.savefig(\n",
    "            f'plots/magnitude_modes_SGLD_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "        )\n",
    "        plt.clf()\n",
    "        fig, ax = plt.subplots()\n",
    "        for i, m in enumerate(magnitude_modes):\n",
    "            ax.plot(\n",
    "                m[: p // 2 + 1],\n",
    "                label=f\"Step {mag_every*i}\",\n",
    "                marker=\"o\",\n",
    "                c=colors[i],\n",
    "                markersize=6,\n",
    "            )\n",
    "        ax.set_xlabel(\"Mode\")\n",
    "        ax.set_ylabel(\"Magnitude\")\n",
    "        ax.legend()\n",
    "        # title\n",
    "        plt.title(\"Fourier mode magnitude vs. SGLD sampling checkpoint\")\n",
    "        fig.savefig(\n",
    "            f'plots/magnitude_modes_SGLD_all_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "        )\n",
    "    return model, lambda_hat\n",
    "\n",
    "\n",
    "def point_sample_pca(colormapping_loss=True):\n",
    "    # get point samples from /point_samples and plot pca in weight space with color corresponding to full loss (use rainbow colormap)\n",
    "    files = glob(\"point_samples/*.json\")\n",
    "    # sort filenames by sgld step\n",
    "    files = sorted(files, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    points = []\n",
    "    losses = []\n",
    "    for f in files:\n",
    "        with open(f, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            points.append(data[\"new_params\"])\n",
    "            losses.append(data[\"full_loss\"])\n",
    "    points = t.tensor(points)\n",
    "    losses = t.tensor(losses)\n",
    "    # normalize points\n",
    "    points = (points - points.mean(dim=0)) / points.std(dim=0)\n",
    "    # fillnan with 0\n",
    "    points[points != points] = 0\n",
    "\n",
    "    print(\"points\", points.shape)\n",
    "    print(\"losses\", losses.shape)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(points)\n",
    "    points_pca = pca.transform(points)\n",
    "    print(\"points_pca\", points_pca.shape)\n",
    "    print(\"explained variance\", pca.explained_variance_ratio_)\n",
    "    print(\"singular values\", pca.singular_values_)\n",
    "    print(\"components\", pca.components_)\n",
    "    print(\"mean\", pca.mean_)\n",
    "    print(\"noise variance\", pca.noise_variance_)\n",
    "    print(\"losses\", losses)\n",
    "\n",
    "    # Plot 200 equally spaced points\n",
    "    points_pca = points_pca[:: len(points_pca) // 200]\n",
    "    losses = losses[:: len(losses) // 200]\n",
    "\n",
    "    # Plot PCA\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Round losses to 2 decimal places\n",
    "    losses = [float(l) for l in losses]\n",
    "\n",
    "    # map color to loss\n",
    "    if colormapping_loss:\n",
    "        cmap = plt.cm.get_cmap(\"rainbow\")\n",
    "        norm = plt.Normalize(vmin=min(losses), vmax=max(losses))\n",
    "        colors = [cmap(norm(l)) for l in losses]\n",
    "    else:\n",
    "        cmap = plt.cm.get_cmap(\"rainbow\")\n",
    "        norm = plt.Normalize(vmin=0, vmax=len(losses))\n",
    "        colors = [cmap(norm(i)) for i in range(len(losses))]\n",
    "\n",
    "    # Plot points with small marker size\n",
    "    ax.scatter(points_pca[:, 0], points_pca[:, 1], c=colors, s=6)\n",
    "\n",
    "    # Label points with loss values\n",
    "    # for i, txt in enumerate([f\"{i}, {round(l, 2)}\" for i, l in enumerate(losses)]):\n",
    "    #     ax.annotate(txt, (points_pca[i][0], points_pca[i][1]), fontsize=7)\n",
    "\n",
    "    # Set x and y labels\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "\n",
    "    fig.savefig(\n",
    "        f'plots/point_samples_pca_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "    )\n",
    "\n",
    "\n",
    "def hyperparameter_search(\n",
    "    params_modular_addition_file,\n",
    "    params_random_file,\n",
    "    n_steps,\n",
    "    m,\n",
    "    epsilon_range,\n",
    "    gamma_range,\n",
    "):\n",
    "    sgld_params = SGLDParams()\n",
    "    sgld_params.m = m\n",
    "    params_modular_addition = ExperimentParams.load_from_file(\n",
    "        params_modular_addition_file\n",
    "    )\n",
    "    params_random = ExperimentParams.load_from_file(params_random_file)\n",
    "    random_dataset = make_random_dataset(params_random.p, params_random.random_seed)\n",
    "    modular_addition_dataset = make_dataset(params_modular_addition.p)\n",
    "    random_dataset, _ = train_test_split(\n",
    "        random_dataset, params_random.train_frac, params_random.random_seed\n",
    "    )\n",
    "    modular_addition_dataset, _ = train_test_split(\n",
    "        modular_addition_dataset,\n",
    "        params_modular_addition.train_frac,\n",
    "        params_modular_addition.random_seed,\n",
    "    )\n",
    "    results_random = defaultdict(list)\n",
    "    results_modular_addition = defaultdict(list)\n",
    "    for epsilon in epsilon_range:\n",
    "        actual_n_steps = int(n_steps / epsilon)\n",
    "        sgld_params.epsilon = epsilon\n",
    "        sgld_params.n_steps = actual_n_steps\n",
    "        for gamma in gamma_range:\n",
    "            mlp_modular_addition = MLP(params_modular_addition)\n",
    "            mlp_random = MLP(params_random)\n",
    "            sgld_params.gamma = gamma\n",
    "            mlp_modular_addition.load_state_dict(\n",
    "                t.load(f\"models/model_{params_modular_addition.get_suffix()}.pt\")\n",
    "            )\n",
    "            mlp_random.load_state_dict(\n",
    "                t.load(f\"models/model_{params_random.get_suffix()}.pt\")\n",
    "            )\n",
    "            _, lambda_hat_modular_addition = sgld(\n",
    "                mlp_modular_addition,\n",
    "                sgld_params,\n",
    "                modular_addition_dataset,\n",
    "                params_modular_addition.device,\n",
    "            )\n",
    "            _, lambda_hat_random = sgld(\n",
    "                mlp_random,\n",
    "                sgld_params,\n",
    "                random_dataset,\n",
    "                params_random.device,\n",
    "            )\n",
    "            results_modular_addition[epsilon].append(lambda_hat_modular_addition)\n",
    "            results_random[epsilon].append(lambda_hat_random)\n",
    "    # plot results\n",
    "    for epsilon in epsilon_range:\n",
    "        plt.clf()\n",
    "        plt.figure()\n",
    "        plt.plot(\n",
    "            gamma_range, results_modular_addition[epsilon], label=\"modular addition\"\n",
    "        )\n",
    "        plt.plot(gamma_range, results_random[epsilon], label=\"random\")\n",
    "        plt.title(\n",
    "            f\"$\\lambda$ vs $\\gamma$ ($\\epsilon$={epsilon}, n_steps={n_steps}, m={m})\"\n",
    "        )\n",
    "        plt.xlabel(\"$\\gamma$\")\n",
    "        plt.ylabel(\"$\\hat{\\lambda}$\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\n",
    "            f'plots/lambda_vs_gamma_epsilon_{epsilon}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "        )\n",
    "        plt.close()\n",
    "    for idx, gamma in enumerate(gamma_range):\n",
    "        plt.clf()\n",
    "        plt.figure()\n",
    "        results_per_epsilon_modular_addition = [\n",
    "            results_modular_addition[epsilon][idx] for epsilon in epsilon_range\n",
    "        ]\n",
    "        results_per_epsilon_random = [\n",
    "            results_random[epsilon][idx] for epsilon in epsilon_range\n",
    "        ]\n",
    "        plt.plot(\n",
    "            epsilon_range,\n",
    "            results_per_epsilon_modular_addition,\n",
    "            label=\"modular addition\",\n",
    "        )\n",
    "        plt.plot(epsilon_range, results_per_epsilon_random, label=\"random\")\n",
    "        plt.title(\n",
    "            f\"$\\lambda$ vs $\\epsilon$ ($\\gamma$={gamma}, n_steps={n_steps}, m={m})\"\n",
    "        )\n",
    "        plt.xlabel(\"$\\epsilon$\")\n",
    "        plt.ylabel(\"$\\hat{\\lambda}$\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\n",
    "            f'plots/lambda_vs_epsilon_gamma_{gamma}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def n_mult_search(\n",
    "    params_modular_addition_file,\n",
    "    params_random_file,\n",
    "    sgld_params,\n",
    "    n_multiplier_range,\n",
    "):\n",
    "    params_modular_addition = ExperimentParams.load_from_file(\n",
    "        params_modular_addition_file\n",
    "    )\n",
    "    params_random = ExperimentParams.load_from_file(params_random_file)\n",
    "    random_dataset = make_random_dataset(params_random.p, params_random.random_seed)\n",
    "    modular_addition_dataset = make_dataset(params_modular_addition.p)\n",
    "    random_dataset, _ = train_test_split(\n",
    "        random_dataset, params_random.train_frac, params_random.random_seed\n",
    "    )\n",
    "    modular_addition_dataset, _ = train_test_split(\n",
    "        modular_addition_dataset,\n",
    "        params_modular_addition.train_frac,\n",
    "        params_modular_addition.random_seed,\n",
    "    )\n",
    "    results_random = []\n",
    "    results_modular_addition = []\n",
    "    for n_mult in n_multiplier_range:\n",
    "        mlp_modular_addition = MLP(params_modular_addition)\n",
    "        mlp_random = MLP(params_random)\n",
    "        sgld_params.n_multiplier = n_mult\n",
    "        mlp_modular_addition.load_state_dict(\n",
    "            t.load(f\"models/model_{params_modular_addition.get_suffix()}.pt\")\n",
    "        )\n",
    "        mlp_random.load_state_dict(\n",
    "            t.load(f\"models/model_{params_random.get_suffix()}.pt\")\n",
    "        )\n",
    "        _, lambda_hat_modular_addition = sgld(\n",
    "            mlp_modular_addition,\n",
    "            sgld_params,\n",
    "            modular_addition_dataset,\n",
    "            params_modular_addition.device,\n",
    "        )\n",
    "        _, lambda_hat_random = sgld(\n",
    "            mlp_random,\n",
    "            sgld_params,\n",
    "            random_dataset,\n",
    "            params_random.device,\n",
    "        )\n",
    "        results_modular_addition.append(lambda_hat_modular_addition)\n",
    "        results_random.append(lambda_hat_random)\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    plt.plot(\n",
    "        n_multiplier_range,\n",
    "        results_modular_addition,\n",
    "        label=\"modular addition\",\n",
    "        marker=\"o\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        n_multiplier_range, results_random, label=\"random\", marker=\"o\", linestyle=\"--\"\n",
    "    )\n",
    "    plt.title(f\"$\\hat\\lambda$ vs n_mult\")\n",
    "    plt.xlabel(\"n_mult\")\n",
    "    plt.ylabel(\"$\\hat{\\lambda}$\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\n",
    "        f'plots/lambda_vs_n_mult_{n_mult}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def get_lambda(params, sgld_params, checkpoint_no=None):\n",
    "    model = MLP(params)\n",
    "    if checkpoint_no is None:\n",
    "        model.load_state_dict(t.load(f\"models/model_{params.get_suffix()}.pt\"))\n",
    "    else:\n",
    "        model.load_state_dict(\n",
    "            t.load(\n",
    "                f\"models/checkpoints/{params.get_suffix(checkpoint_no=checkpoint_no)}.pt\"\n",
    "            )\n",
    "        )\n",
    "    if params.use_random_dataset:\n",
    "        dataset = make_random_dataset(params.p, params.random_seed)\n",
    "    else:\n",
    "        dataset = make_dataset(params.a, params.b, params.p, params.num_exceptions, params.use_exceptions)\n",
    "    train_data, test_data = train_test_split(\n",
    "        dataset, params.train_frac, params.random_seed\n",
    "    )\n",
    "    test_loss = get_full_train_loss(model, test_data, params.device)\n",
    "    train_loss = get_full_train_loss(model, train_data, params.device)\n",
    "    _, lambda_hat = sgld(model, sgld_params, train_data, params.device)\n",
    "    lambda_hat, test_loss, train_loss = lambda_hat.cpu().item(), test_loss.cpu().item(), train_loss.cpu().item()\n",
    "    return lambda_hat, test_loss, train_loss\n",
    "\n",
    "\n",
    "def get_lambda_per_quantity(param_files, sgld_params, resample=True):\n",
    "    lambda_values = []\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    for param_file in param_files:\n",
    "        params = ExperimentParams.load_from_file(param_file)\n",
    "        if not resample and params.lambda_hat is not None:\n",
    "            lambda_values.append(params.lambda_hat)\n",
    "            continue\n",
    "        lambda_hat, test_loss, train_loss = get_lambda(params, sgld_params)\n",
    "        lambda_values.append(lambda_hat)\n",
    "        test_losses.append(test_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        param_dict = params.get_dict()\n",
    "        param_dict[\"lambda_hat\"] = lambda_hat\n",
    "        param_dict[\"test_loss\"] = test_loss\n",
    "        param_dict[\"train_loss\"] = train_loss\n",
    "        with open(param_file, \"w\") as f:\n",
    "            json.dump(param_dict, f)\n",
    "    return lambda_values, test_losses, train_losses\n",
    "\n",
    "\n",
    "def plot_lambda_test_train_loss(\n",
    "    ax1, x_axis, x_label, lambda_values, test_losses, train_losses\n",
    "):\n",
    "    # Plot lambda values on the left y-axis\n",
    "    ax1.plot(x_axis, lambda_values, marker=\"o\", label=\"$\\hat{\\lambda}$\", color=\"g\")\n",
    "    # ax1.plot(x_axis, [8 * x for x in x_axis], label=\"y=8x\", linestyle=\"--\")\n",
    "    ax1.set_xlabel(x_label)\n",
    "    ax1.set_ylabel(\"$\\hat{\\lambda}$\")\n",
    "    ax1.tick_params(\"y\", colors=\"g\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "\n",
    "    # Create a second y-axis for the losses\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(\n",
    "        x_axis, train_losses, marker=\"o\", color=\"b\", label=\"train loss\", linestyle=\"--\"\n",
    "    )\n",
    "    ax2.plot(\n",
    "        x_axis, test_losses, marker=\"o\", color=\"r\", label=\"test loss\", linestyle=\"--\"\n",
    "    )\n",
    "    ax2.set_ylabel(\"Loss\", color=\"b\")\n",
    "    ax2.tick_params(\"y\", colors=\"b\")\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "def plot_lambda_per_quantity(param_files, quantity_values, quantity_name, sgld_params):\n",
    "    lambda_values, test_losses, train_losses = get_lambda_per_quantity(\n",
    "        param_files, sgld_params\n",
    "    )\n",
    "\n",
    "    # Clear previous plots\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    plot_lambda_test_train_loss(\n",
    "        ax1, quantity_values, quantity_name, lambda_values, test_losses, train_losses\n",
    "    )\n",
    "\n",
    "    # Set title\n",
    "    ax1.set_title(f\"$\\lambda$ vs {quantity_name}\")\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(\n",
    "        f'plots/lambda_vs_{quantity_name.replace(\" \", \"_\")}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "    )\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_lambda_per_checkpoint(param_file, sgld_params, checkpoints=None):\n",
    "    lambda_values = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    params = ExperimentParams.load_from_file(param_file)\n",
    "    check_list = list(range(params.n_save_model_checkpoints))\n",
    "    if checkpoints is not None:\n",
    "        check_list = checkpoints\n",
    "    for i in check_list:\n",
    "        lambda_hat, test_loss, train_loss = get_lambda(\n",
    "            params, sgld_params, checkpoint_no=i\n",
    "        )\n",
    "        lambda_values.append(lambda_hat)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    # Clear previous plots\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot lambda values on the left y-axis\n",
    "    plot_lambda_test_train_loss(\n",
    "        ax1, check_list, \"Checkpoint\", lambda_values, test_losses, train_losses\n",
    "    )\n",
    "\n",
    "    # Set title\n",
    "    title = \"$\\lambda$ vs checkpoint\"\n",
    "    if sgld_params.restrict_to_orth_grad:\n",
    "        title += \" (restrict orth dir)\"\n",
    "    if sgld_params.n_multiplier != 1:\n",
    "        title += f\" (n*={sgld_params.n_multiplier})\"\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(\n",
    "        f'plots/lambda_vs_checkpoint_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}_restrictorth{sgld_params.restrict_to_orth_grad}.png'\n",
    "    )\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_lambda_per_checkpoint_multi_run(sgld_params, sweep_dir):\n",
    "    files = glob(f\"{sweep_dir}/*.json\")\n",
    "\n",
    "    results = defaultdict(dict)\n",
    "    for f in files:\n",
    "        exp_params = ExperimentParams.load_from_file(f)\n",
    "        run_id = exp_params.run_id\n",
    "        n_checkpoints = exp_params.n_save_model_checkpoints\n",
    "        check_list = list(range(n_checkpoints))\n",
    "        for c in check_list:\n",
    "            lambda_hat, test_loss, train_loss = get_lambda(\n",
    "                exp_params, sgld_params, checkpoint_no=c\n",
    "            )\n",
    "            results[run_id][c] = (lambda_hat, test_loss, train_loss)\n",
    "\n",
    "    # Clear previous plots\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    # Plot mean lambda, test loss, and train loss per checkpoint (mean over runs)\n",
    "    run_ids = sorted(list(results.keys()))\n",
    "    lambda_values = []\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    for c in check_list:\n",
    "        lambda_values.append(t.mean(t.tensor([run[c][0] for run in results.values()])))\n",
    "        test_losses.append(t.mean(t.tensor([run[c][1] for run in results.values()])))\n",
    "        train_losses.append(t.mean(t.tensor([run[c][2] for run in results.values()])))\n",
    "    plot_lambda_test_train_loss(\n",
    "        ax1, check_list, \"Checkpoint\", lambda_values, test_losses, train_losses\n",
    "    )\n",
    "\n",
    "    # Set title\n",
    "    title = \"$\\lambda$ vs checkpoint\"\n",
    "    if sgld_params.restrict_to_orth_grad:\n",
    "        title += \" (restrict orth dir)\"\n",
    "    if sgld_params.n_multiplier != 1:\n",
    "        title += f\" (n*={sgld_params.n_multiplier})\"\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(\n",
    "        f'plots/lambda_vs_checkpoint_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}_restrictorth{sgld_params.restrict_to_orth_grad}_meanover_{len(run_ids)}.png'\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_lambda_per_p(sgld_params, p_sweep_dir, resample=False, append_to_title=\"\"):\n",
    "    files = glob(f\"{p_sweep_dir}/*.json\")\n",
    "    results = defaultdict(dict)\n",
    "    for f in files:\n",
    "        exp_params = ExperimentParams.load_from_file(f)\n",
    "        p = exp_params.p\n",
    "        run_id = exp_params.run_id\n",
    "        if not resample and all(\n",
    "            [\n",
    "                exp_params.lambda_hat is not None,\n",
    "                exp_params.test_loss is not None,\n",
    "                exp_params.train_loss is not None,\n",
    "            ]\n",
    "        ):\n",
    "            results[p][run_id] = (\n",
    "                exp_params.lambda_hat,\n",
    "                exp_params.test_loss,\n",
    "                exp_params.train_loss,\n",
    "            )\n",
    "            continue\n",
    "        lambda_hat, test_loss, train_loss = get_lambda(exp_params, sgld_params)\n",
    "        results[p][run_id] = (lambda_hat, test_loss, train_loss)\n",
    "        exp_params.lambda_hat = lambda_hat\n",
    "        exp_params.test_loss = test_loss\n",
    "        exp_params.train_loss = train_loss\n",
    "        exp_params.save_to_file(f)\n",
    "    print(\"Extracted results\", results)\n",
    "\n",
    "    # Plot average lambda, test loss, and train loss per p (mean over runs)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    p_values = sorted(list(results.keys()))\n",
    "    lambda_values = []\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    for p in p_values:\n",
    "        lambda_values.append(t.mean(t.tensor([run[0] for run in results[p].values()])))\n",
    "        test_losses.append(t.mean(t.tensor([run[1] for run in results[p].values()])))\n",
    "        train_losses.append(t.mean(t.tensor([run[2] for run in results[p].values()])))\n",
    "    plot_lambda_test_train_loss(\n",
    "        ax1, p_values, \"p\", lambda_values, test_losses, train_losses\n",
    "    )\n",
    "    ax1.set_title(\"$\\lambda$ vs p\")\n",
    "    fig.savefig(\n",
    "        f'plots/lambda_vs_p_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}_{append_to_title}.png'\n",
    "    )\n",
    "    print(\"Saved plot 1\")\n",
    "\n",
    "    # Plot lambda (each run is a different color)\n",
    "    run_ids = sorted(list(results[p_values[0]].keys()))\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    for run_id in run_ids:\n",
    "        lambda_values = []\n",
    "        for p in p_values:\n",
    "            lambda_values.append(results[p][run_id][0])\n",
    "        ax1.plot(p_values, lambda_values, marker=\"o\", label=f\"Run {run_id}\")\n",
    "    ax1.set_xlabel(\"p\")\n",
    "    ax1.set_ylabel(\"$\\hat{\\lambda}$\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.set_title(\"$\\lambda$ vs p\")\n",
    "    fig.savefig(\n",
    "        f'plots/lambda_vs_p_runs_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "    )\n",
    "    print(\"Saved plot 2\")\n",
    "\n",
    "\n",
    "def plot_lambda_per_p_different_exps(exp_dirs, exp_names, sgld_params, resample=False):\n",
    "    \"\"\"\n",
    "    exp_dirs: list of directories containing experiment params json files corresponding to different p value sweeps\n",
    "    exp_names: name of experiment being run for that p sweep\n",
    "    sgld_params: List of SGLDParams objects, or single settings - SGLD settings to use for each exp_dir\n",
    "    resample: whether to resample if lambda already saved\n",
    "    \"\"\"\n",
    "    # If sgld_params is not a list, make it a list\n",
    "    if not isinstance(sgld_params, list):\n",
    "        sgld_params = [sgld_params] * len(exp_dirs)\n",
    "    results = defaultdict(lambda: defaultdict(list))\n",
    "    for i, d in enumerate(exp_dirs):\n",
    "        files = glob(f\"{d}/*.json\")\n",
    "        for f in files:\n",
    "            exp_params = ExperimentParams.load_from_file(f)\n",
    "            p = exp_params.p\n",
    "            if not resample and exp_params.lambda_hat is not None:\n",
    "                results[exp_names[i]][p].append(exp_params.lambda_hat)\n",
    "            else:\n",
    "                lambda_hat, _, _ = get_lambda(exp_params, sgld_params[i])\n",
    "                results[exp_names[i]][p].append(lambda_hat)\n",
    "                exp_params.lambda_hat = lambda_hat\n",
    "                exp_params.save_to_file(f)\n",
    "\n",
    "    # lambda per p for each exp on same plot\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    for n in exp_names:\n",
    "        p_values = sorted(list(results[n].keys()))\n",
    "        lambda_values = []\n",
    "        for p in p_values:\n",
    "            l = sum(results[n][p]) / len(results[n][p])\n",
    "            lambda_values.append(l)\n",
    "        plt.plot(p_values, lambda_values, marker=\"o\", label=n, linestyle=\"--\")\n",
    "    plt.xlabel(\"p\")\n",
    "    plt.ylabel(\"$\\hat{\\lambda}$\")\n",
    "    plt.legend()\n",
    "    plt.title(\"$\\lambda$ vs p\")\n",
    "    fig.savefig(f'plots/lambda_vs_p_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png')\n",
    "\n",
    "\n",
    "def plot_lambda_per_frac(sgld_params, frac_sweep_dir, resample=False):\n",
    "    files = glob(f\"{frac_sweep_dir}/*.json\")\n",
    "    results = defaultdict(dict)\n",
    "    for f in files:\n",
    "        exp_params = ExperimentParams.load_from_file(f)\n",
    "        frac = exp_params.train_frac\n",
    "        run_id = exp_params.run_id\n",
    "        if not resample and all(\n",
    "            [\n",
    "                exp_params.lambda_hat is not None,\n",
    "                exp_params.test_loss is not None,\n",
    "                exp_params.train_loss is not None,\n",
    "            ]\n",
    "        ):\n",
    "            results[frac][run_id] = (\n",
    "                exp_params.lambda_hat,\n",
    "                exp_params.test_loss,\n",
    "                exp_params.train_loss,\n",
    "            )\n",
    "            continue\n",
    "        lambda_hat, test_loss, train_loss = get_lambda(exp_params, sgld_params)\n",
    "        results[frac][run_id] = (lambda_hat, test_loss, train_loss)\n",
    "        exp_params.lambda_hat = lambda_hat\n",
    "        exp_params.test_loss = test_loss\n",
    "        exp_params.train_loss = train_loss\n",
    "        exp_params.save_to_file(f)\n",
    "    print(\"Extracted results\", results)\n",
    "\n",
    "    # Plot average lambda, test loss, and train loss per p (mean over runs)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    frac_values = sorted(list(results.keys()))\n",
    "    lambda_values = []\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    for frac in frac_values:\n",
    "        lambda_values.append(\n",
    "            t.mean(t.tensor([run[0] for run in results[frac].values()]))\n",
    "        )\n",
    "        test_losses.append(t.mean(t.tensor([run[1] for run in results[frac].values()])))\n",
    "        train_losses.append(\n",
    "            t.mean(t.tensor([run[2] for run in results[frac].values()]))\n",
    "        )\n",
    "    plot_lambda_test_train_loss(\n",
    "        ax1, frac_values, \"train_frac\", lambda_values, test_losses, train_losses\n",
    "    )\n",
    "    ax1.set_title(\"$\\lambda$ vs train_frac\")\n",
    "    fig.savefig(f'plots/lambda_vs_frac_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png')\n",
    "\n",
    "\n",
    "def slope_vs_n_mult(sgld_params, exp_dir, n_mults):\n",
    "    files = glob(f\"{exp_dir}/*.json\")\n",
    "    slopes = []\n",
    "    for n_mult in n_mults:\n",
    "        sgld_params.n_multiplier = n_mult\n",
    "        results = defaultdict(dict)\n",
    "        for f in files:\n",
    "            exp_params = ExperimentParams.load_from_file(f)\n",
    "            p = exp_params.p\n",
    "            run_id = exp_params.run_id\n",
    "            lambda_hat, test_loss, train_loss = get_lambda(exp_params, sgld_params)\n",
    "            results[p][run_id] = (\n",
    "                lambda_hat,\n",
    "                test_loss,\n",
    "                train_loss,\n",
    "            )\n",
    "            exp_params.lambda_hat = lambda_hat\n",
    "            exp_params.test_loss = test_loss\n",
    "            exp_params.train_loss = train_loss\n",
    "            exp_params.save_to_file(f)\n",
    "\n",
    "        with open(f\"results/results_n_mult_{n_mult}.json\", \"w\") as f:\n",
    "            json.dump(dict(results), f)\n",
    "\n",
    "        avg_over_runs = defaultdict(list)\n",
    "        for p in results.keys():\n",
    "            for run_id in results[p].keys():\n",
    "                avg_over_runs[p].append(results[p][run_id][0])\n",
    "        for p in avg_over_runs.keys():\n",
    "            avg_over_runs[p] = sum(avg_over_runs[p]) / len(avg_over_runs[p])\n",
    "        p1 = list(avg_over_runs.keys())[0]\n",
    "        p2 = list(avg_over_runs.keys())[-1]\n",
    "        avg_lambda_p1 = avg_over_runs[p1]\n",
    "        avg_lambda_p2 = avg_over_runs[p2]\n",
    "        slope = abs((avg_lambda_p2 - avg_lambda_p1) / (p2 - p1))\n",
    "        slopes.append(slope)\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    plt.plot(n_mults, slopes, marker=\"o\")\n",
    "    plt.title(\"Slope of $\\lambda$ vs p\")\n",
    "    plt.xlabel(\"n_multiplier\")\n",
    "    plt.ylabel(\"Slope\")\n",
    "    # log scale on x axis\n",
    "    plt.xscale(\"log\")\n",
    "    plt.savefig(\n",
    "        f'plots/slope_lambda_vs_p_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def grok_exp():\n",
    "    temp_multipliers = [0.1, 0.3, 1, 3]\n",
    "    files = {\n",
    "        2: list(glob(\"exp_params/QUADRATIC/*2grok.json\")),\n",
    "        3: list(glob(\"exp_params/QUADRATIC/*3grok.json\")),\n",
    "        4: list(glob(\"exp_params/QUADRATIC/*4grok.json\")),\n",
    "        # 5: list(glob(\"exp_params/rerun_exps/*5grok.json\")),\n",
    "    }\n",
    "    full_results = []\n",
    "    plt.clf()\n",
    "    for tm in temp_multipliers:\n",
    "        sgld_params = SGLDParams(\n",
    "            gamma=5,\n",
    "            epsilon=0.0001,\n",
    "            n_steps=5000,\n",
    "            m=200,\n",
    "            temp_multiplier=tm,\n",
    "            get_updated_model_parameters=lambda m: m.embedding.parameters(),\n",
    "        )\n",
    "        results = []\n",
    "        for n_groks, filenames in files.items():\n",
    "            lambdas = []\n",
    "            for f in filenames:\n",
    "                exp_params = ExperimentParams.load_from_file(f)\n",
    "                lambda_hat, _, _ = get_lambda(exp_params, sgld_params)\n",
    "                lambdas.append(lambda_hat)\n",
    "            mean_lambda = sum(lambdas) / len(lambdas)\n",
    "            results.append(mean_lambda)\n",
    "        full_results.append(results)\n",
    "        plt.plot(\n",
    "            list(files.keys()),\n",
    "            results,\n",
    "            marker=\"o\",\n",
    "            label=f\"temp_multiplier={tm}\",\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "        print(f\"temp_multiplier={tm}, results={results}\")\n",
    "    print(f\"full_results={full_results}\")\n",
    "    plt.xlabel(\"# circuits\")\n",
    "    plt.ylabel(\"$\\hat{\\lambda}$\")\n",
    "    plt.xticks(list(files.keys()))\n",
    "    plt.legend()\n",
    "    plt.title(\"$\\hat{\\lambda}$ vs # circuits at different temperatures\")\n",
    "    plt.savefig(\n",
    "        f'plots/lambda_vs_n_groks_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sgld_params = SGLDParams(\n",
    "        gamma=5,\n",
    "        epsilon=0.0001,\n",
    "        n_steps=2000,\n",
    "        m=64,\n",
    "        restrict_to_orth_grad=True,\n",
    "        weight_decay=0.0\n",
    "    )\n",
    "    # plot_lambda_per_checkpoint(\"models/params_P77_frac0.95_hid32_emb16_tieunembedFalse_tielinFalse_freezeFalse_run0.json\", params)\n",
    "    param_file_False = \"models/params_P143_exceptions=False_num_exceptions=5_a=13_b=11.json\"\n",
    "    params_False = ExperimentParams.load_from_file(param_file_False)\n",
    "\n",
    "    if params_False.use_exceptions != False:\n",
    "        raise ValueError(\"Expected params_False.use_exceptions to be False, but got True.\")\n",
    "        \n",
    "    check_list_1 = list(range(params_False.n_save_model_checkpoints))\n",
    "    lambda_hat_False, test_loss_1, train_loss_1 = get_lambda(params_False, sgld_params, check_list_1[-1])\n",
    "    \n",
    "    param_file_True = \"models/params_P143_exceptions=True_num_exceptions=5_a=13_b=11.json\"\n",
    "    params_True = ExperimentParams.load_from_file(param_file_True)\n",
    "\n",
    "    if params_True.use_exceptions != True:\n",
    "        raise ValueError(\"Expected params_True.use_exceptions to be True, but got False.\")\n",
    "        \n",
    "    check_list_True = list(range(params_True.n_save_model_checkpoints))\n",
    "    \n",
    "    lambda_hat_True, test_loss_2, train_loss_2 = get_lambda(params_True, sgld_params, check_list_True[-1])\n",
    "    print(f\"\\n\\n\\n\\nExceptions False Lhat: {lambda_hat_False}\")\n",
    "    print(f\"Exceptions True Lhat: {lambda_hat_True}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "df-stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
